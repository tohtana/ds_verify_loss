NUM_NODES: 1 NGPUS_PER_NODE: 8 NUM_PROCESSES: 8
HOST_IP: 127.0.0.1
NUM_NODES: 1
NUM_PROCESSES: 8
BACKEND: deepspeed
ZERO_STAGE: 1
MODEL: meta-llama/Meta-Llama-3-8B
GRADIENT_ACCUMULATION_STEPS: 4
EXTRA_OPTS:  --gradient_accumulation_steps 4 --dataset_percentage 20.0 --use_wandb
Logging to logs/debug_n0_Meta-Llama-3-8B_deepspeed_np8z1c0dc0E0b1seq512g4a1pALL.log
[2025-06-17 03:51:20,299] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 03:51:20,381] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 03:51:21,905] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
W0617 03:51:22.458000 350796 site-packages/torch/distributed/run.py:766] 
W0617 03:51:22.458000 350796 site-packages/torch/distributed/run.py:766] *****************************************
W0617 03:51:22.458000 350796 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0617 03:51:22.458000 350796 site-packages/torch/distributed/run.py:766] *****************************************
[W617 03:51:22.307376581 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W617 03:51:22.307401233 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [fdff:ffff:2c75::2464:0]:25550 (errno: 97 - Address family not supported by protocol).
Namespace(model_name='meta-llama/Meta-Llama-3-8B', batch_size=1, num_epochs=5, seq_length=512, learning_rate=1e-06, max_grad_norm=1.0, gradient_accumulation_steps=4, activation_checkpointing=False, eval=False, dataset_name='wikitext', dataset_percentage=20.0, num_layers=0, attn_impl='sdpa', compile=False, passes=None, backend='inductor', offload_opt_states=False, profile=False, deterministic=False, profile_dir=None, bench_step=100, warmup_step=15, zero_stage=1, log_interval=10, save_weights=False, load_weights=False, use_wandb=True, wandb_project='ds-verify-loss', wandb_run_name=None, wandb_tags=[])
Namespace(model_name='meta-llama/Meta-Llama-3-8B', batch_size=1, num_epochs=5, seq_length=512, learning_rate=1e-06, max_grad_norm=1.0, gradient_accumulation_steps=4, activation_checkpointing=False, eval=False, dataset_name='wikitext', dataset_percentage=20.0, num_layers=0, attn_impl='sdpa', compile=False, passes=None, backend='inductor', offload_opt_states=False, profile=False, deterministic=False, profile_dir=None, bench_step=100, warmup_step=15, zero_stage=1, log_interval=10, save_weights=False, load_weights=False, use_wandb=True, wandb_project='ds-verify-loss', wandb_run_name=None, wandb_tags=[])Namespace(model_name='meta-llama/Meta-Llama-3-8B', batch_size=1, num_epochs=5, seq_length=512, learning_rate=1e-06, max_grad_norm=1.0, gradient_accumulation_steps=4, activation_checkpointing=False, eval=False, dataset_name='wikitext', dataset_percentage=20.0, num_layers=0, attn_impl='sdpa', compile=False, passes=None, backend='inductor', offload_opt_states=False, profile=False, deterministic=False, profile_dir=None, bench_step=100, warmup_step=15, zero_stage=1, log_interval=10, save_weights=False, load_weights=False, use_wandb=True, wandb_project='ds-verify-loss', wandb_run_name=None, wandb_tags=[])Namespace(model_name='meta-llama/Meta-Llama-3-8B', batch_size=1, num_epochs=5, seq_length=512, learning_rate=1e-06, max_grad_norm=1.0, gradient_accumulation_steps=4, activation_checkpointing=False, eval=False, dataset_name='wikitext', dataset_percentage=20.0, num_layers=0, attn_impl='sdpa', compile=False, passes=None, backend='inductor', offload_opt_states=False, profile=False, deterministic=False, profile_dir=None, bench_step=100, warmup_step=15, zero_stage=1, log_interval=10, save_weights=False, load_weights=False, use_wandb=True, wandb_project='ds-verify-loss', wandb_run_name=None, wandb_tags=[])


Namespace(model_name='meta-llama/Meta-Llama-3-8B', batch_size=1, num_epochs=5, seq_length=512, learning_rate=1e-06, max_grad_norm=1.0, gradient_accumulation_steps=4, activation_checkpointing=False, eval=False, dataset_name='wikitext', dataset_percentage=20.0, num_layers=0, attn_impl='sdpa', compile=False, passes=None, backend='inductor', offload_opt_states=False, profile=False, deterministic=False, profile_dir=None, bench_step=100, warmup_step=15, zero_stage=1, log_interval=10, save_weights=False, load_weights=False, use_wandb=True, wandb_project='ds-verify-loss', wandb_run_name=None, wandb_tags=[])
Namespace(model_name='meta-llama/Meta-Llama-3-8B', batch_size=1, num_epochs=5, seq_length=512, learning_rate=1e-06, max_grad_norm=1.0, gradient_accumulation_steps=4, activation_checkpointing=False, eval=False, dataset_name='wikitext', dataset_percentage=20.0, num_layers=0, attn_impl='sdpa', compile=False, passes=None, backend='inductor', offload_opt_states=False, profile=False, deterministic=False, profile_dir=None, bench_step=100, warmup_step=15, zero_stage=1, log_interval=10, save_weights=False, load_weights=False, use_wandb=True, wandb_project='ds-verify-loss', wandb_run_name=None, wandb_tags=[])
Namespace(model_name='meta-llama/Meta-Llama-3-8B', batch_size=1, num_epochs=5, seq_length=512, learning_rate=1e-06, max_grad_norm=1.0, gradient_accumulation_steps=4, activation_checkpointing=False, eval=False, dataset_name='wikitext', dataset_percentage=20.0, num_layers=0, attn_impl='sdpa', compile=False, passes=None, backend='inductor', offload_opt_states=False, profile=False, deterministic=False, profile_dir=None, bench_step=100, warmup_step=15, zero_stage=1, log_interval=10, save_weights=False, load_weights=False, use_wandb=True, wandb_project='ds-verify-loss', wandb_run_name=None, wandb_tags=[])
Namespace(model_name='meta-llama/Meta-Llama-3-8B', batch_size=1, num_epochs=5, seq_length=512, learning_rate=1e-06, max_grad_norm=1.0, gradient_accumulation_steps=4, activation_checkpointing=False, eval=False, dataset_name='wikitext', dataset_percentage=20.0, num_layers=0, attn_impl='sdpa', compile=False, passes=None, backend='inductor', offload_opt_states=False, profile=False, deterministic=False, profile_dir=None, bench_step=100, warmup_step=15, zero_stage=1, log_interval=10, save_weights=False, load_weights=False, use_wandb=True, wandb_project='ds-verify-loss', wandb_run_name=None, wandb_tags=[])
[2025-06-17 03:51:28,562] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 03:51:28,665] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 03:51:28,781] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 03:51:28,851] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 03:51:28,896] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 03:51:28,908] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 03:51:28,917] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 03:51:28,945] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 03:51:28,947] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 03:51:28,950] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 03:51:29,098] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 03:51:29,142] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 03:51:29,177] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 03:51:29,221] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 03:51:29,279] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 03:51:29,291] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 03:51:29,672] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-17 03:51:29,896] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-17 03:51:30,004] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-17 03:51:30,117] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-17 03:51:30,152] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-17 03:51:30,180] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-17 03:51:30,249] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-06-17 03:51:30,253] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[W617 03:51:30.960817601 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W617 03:51:30.960852823 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [fdff:ffff:8e7c:0:2c59:8cba:f464:0]:60528 (errno: 97 - Address family not supported by protocol).
[2025-06-17 03:51:30,292] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-17 03:51:30,526] [INFO] [comm.py:675:init_distributed] cdb=None
[W617 03:51:30.237876302 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W617 03:51:30.237908623 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [fdff:ffff:907e:0:2cf9:f2d2:6c57:0]:4309 (errno: 97 - Address family not supported by protocol).
Running on device: cuda:5 is_deepspeed: True
[2025-06-17 03:51:30,729] [INFO] [comm.py:675:init_distributed] cdb=None
[W617 03:51:30.440577286 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W617 03:51:30.440616833 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [fdff:ffff:c974:0:2c89:61ca:15a:0]:4729 (errno: 97 - Address family not supported by protocol).
[2025-06-17 03:51:30,774] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-06-17 03:51:30,774] [INFO] [comm.py:706:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W617 03:51:30.486650722 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W617 03:51:30.486698952 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [fdff:ffff:d473:0:2c09:e6ba:8f55:0]:59652 (errno: 97 - Address family not supported by protocol).
[2025-06-17 03:51:30,834] [INFO] [comm.py:675:init_distributed] cdb=None
[W617 03:51:30.548854331 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W617 03:51:30.548894937 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [fdff:ffff:e77e:0:2ca9:17d3:d55b:0]:57423 (errno: 97 - Address family not supported by protocol).
[2025-06-17 03:51:30,904] [INFO] [comm.py:675:init_distributed] cdb=None
[W617 03:51:30.615359975 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W617 03:51:30.615391334 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [fdff:ffff:867a:0:2cf9:494d:e5a:0]:27834 (errno: 97 - Address family not supported by protocol).
[2025-06-17 03:51:30,966] [INFO] [comm.py:675:init_distributed] cdb=None
[W617 03:51:30.677702450 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W617 03:51:30.677736834 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [fdff:ffff:d272:0:2cb9:3d35:f55e:0]:29072 (errno: 97 - Address family not supported by protocol).
[2025-06-17 03:51:30,983] [INFO] [comm.py:675:init_distributed] cdb=None
[W617 03:51:30.694036085 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W617 03:51:30.694066415 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [fdff:ffff:f27f:0:2c59:1da:8f57:0]:600 (errno: 97 - Address family not supported by protocol).
Running on device: cuda:4 is_deepspeed: True
Running on device: cuda:0 is_deepspeed: True
Loading model and tokenizer...
Running on device: cuda:3 is_deepspeed: True
Running on device: cuda:1 is_deepspeed: True
Running on device: cuda:6 is_deepspeed: True
Running on device: cuda:7 is_deepspeed: True
Running on device: cuda:2 is_deepspeed: True
Loading dataset: wikitext (20.0% of data)...
Dataset loaded: 360270 examples using column 'text'
Tokenizing dataset...
Map:   0%|          | 0/360270 [00:00<?, ? examples/s]Map:   0%|          | 1000/360270 [00:00<00:50, 7086.60 examples/s]Map:   1%|          | 2000/360270 [00:00<00:53, 6719.23 examples/s]Map:   1%|          | 3000/360270 [00:00<00:55, 6416.00 examples/s]Map:   1%|          | 4000/360270 [00:00<00:57, 6216.01 examples/s]Map:   1%|▏         | 5000/360270 [00:00<00:56, 6263.98 examples/s]Map:   2%|▏         | 6000/360270 [00:00<00:58, 6065.39 examples/s]Map:   2%|▏         | 7000/360270 [00:01<00:58, 6013.79 examples/s]Map:   2%|▏         | 8000/360270 [00:01<00:58, 6061.40 examples/s]Map:   2%|▏         | 9000/360270 [00:01<00:59, 5941.03 examples/s]Map:   3%|▎         | 10000/360270 [00:01<00:59, 5879.81 examples/s]Map:   3%|▎         | 11000/360270 [00:01<00:57, 6028.44 examples/s]Map:   3%|▎         | 12000/360270 [00:01<01:00, 5760.38 examples/s]Map:   4%|▎         | 13000/360270 [00:02<00:58, 5898.37 examples/s]Map:   4%|▍         | 14000/360270 [00:02<00:58, 5939.29 examples/s]Map:   4%|▍         | 15000/360270 [00:02<00:59, 5772.86 examples/s]Map:   4%|▍         | 16000/360270 [00:02<01:00, 5701.53 examples/s]Map:   5%|▍         | 17000/360270 [00:02<01:00, 5654.59 examples/s]Map:   5%|▍         | 18000/360270 [00:03<01:01, 5528.34 examples/s]Map:   5%|▌         | 19000/360270 [00:03<01:02, 5493.22 examples/s]Map:   6%|▌         | 20000/360270 [00:03<01:03, 5367.20 examples/s]Map:   6%|▌         | 21000/360270 [00:03<01:04, 5241.82 examples/s]Map:   6%|▌         | 22000/360270 [00:03<01:06, 5090.44 examples/s]Map:   6%|▋         | 23000/360270 [00:04<01:06, 5073.10 examples/s]Map:   7%|▋         | 24000/360270 [00:04<01:12, 4621.57 examples/s]Map:   7%|▋         | 25000/360270 [00:04<01:11, 4715.23 examples/s]Map:   7%|▋         | 26000/360270 [00:04<01:11, 4653.94 examples/s]Map:   7%|▋         | 27000/360270 [00:04<01:10, 4759.20 examples/s]Map:   8%|▊         | 28000/360270 [00:05<01:08, 4843.29 examples/s]Map:   8%|▊         | 29000/360270 [00:05<01:06, 4992.21 examples/s]Map:   8%|▊         | 30000/360270 [00:05<01:53, 2920.75 examples/s]Map:   9%|▊         | 31000/360270 [00:06<01:38, 3344.48 examples/s]Map:   9%|▉         | 32000/360270 [00:06<01:27, 3764.75 examples/s]Map:   9%|▉         | 33000/360270 [00:06<01:21, 3999.30 examples/s]Map:   9%|▉         | 34000/360270 [00:06<01:17, 4193.48 examples/s]Map:  10%|▉         | 35000/360270 [00:06<01:11, 4521.18 examples/s]Map:  10%|▉         | 36000/360270 [00:07<01:10, 4581.74 examples/s]Map:  10%|█         | 37000/360270 [00:07<01:14, 4363.76 examples/s]Map:  11%|█         | 38000/360270 [00:07<01:14, 4311.73 examples/s]Map:  11%|█         | 39000/360270 [00:07<01:13, 4358.31 examples/s]Map:  11%|█         | 40000/360270 [00:08<01:16, 4190.85 examples/s]Map:  11%|█▏        | 41000/360270 [00:08<01:14, 4289.40 examples/s]Map:  12%|█▏        | 42000/360270 [00:08<01:14, 4275.29 examples/s]Map:  12%|█▏        | 43000/360270 [00:08<01:17, 4101.85 examples/s]Map:  12%|█▏        | 44000/360270 [00:09<01:12, 4375.05 examples/s]Map:  12%|█▏        | 45000/360270 [00:09<01:09, 4515.61 examples/s]Map:  13%|█▎        | 46000/360270 [00:09<01:09, 4522.36 examples/s]Map:  13%|█▎        | 47000/360270 [00:09<01:20, 3915.83 examples/s]Map:  13%|█▎        | 48000/360270 [00:10<01:13, 4259.95 examples/s]Map:  14%|█▎        | 49000/360270 [00:10<01:13, 4246.24 examples/s]Map:  14%|█▍        | 50000/360270 [00:10<01:10, 4422.73 examples/s]Map:  14%|█▍        | 51000/360270 [00:10<01:11, 4327.57 examples/s]Map:  14%|█▍        | 52000/360270 [00:10<01:09, 4456.15 examples/s]Map:  15%|█▍        | 53000/360270 [00:11<01:07, 4579.00 examples/s]Map:  15%|█▍        | 54000/360270 [00:11<01:14, 4124.58 examples/s]Map:  15%|█▌        | 55000/360270 [00:11<01:17, 3932.52 examples/s]Map:  16%|█▌        | 56000/360270 [00:11<01:17, 3925.12 examples/s]Map:  16%|█▌        | 57000/360270 [00:12<01:16, 3941.47 examples/s]Map:  16%|█▌        | 58000/360270 [00:12<01:17, 3903.35 examples/s]Map:  16%|█▋        | 59000/360270 [00:12<01:13, 4084.29 examples/s]Map:  17%|█▋        | 60000/360270 [00:12<01:15, 3982.20 examples/s]Map:  17%|█▋        | 61000/360270 [00:13<01:10, 4254.11 examples/s]Map:  17%|█▋        | 62000/360270 [00:13<01:06, 4465.24 examples/s]Map:  17%|█▋        | 63000/360270 [00:13<01:07, 4424.99 examples/s]Map:  18%|█▊        | 64000/360270 [00:13<01:06, 4422.15 examples/s]Map:  18%|█▊        | 65000/360270 [00:14<01:05, 4526.35 examples/s]Map:  18%|█▊        | 66000/360270 [00:14<01:55, 2553.73 examples/s]Map:  19%|█▊        | 67000/360270 [00:15<01:39, 2941.68 examples/s]Map:  19%|█▉        | 68000/360270 [00:15<01:31, 3209.61 examples/s]Map:  19%|█▉        | 69000/360270 [00:15<01:22, 3544.29 examples/s]Map:  19%|█▉        | 70000/360270 [00:15<01:15, 3861.48 examples/s]Map:  20%|█▉        | 71000/360270 [00:15<01:08, 4224.64 examples/s]Map:  20%|█▉        | 72000/360270 [00:16<01:05, 4400.91 examples/s]Map:  20%|██        | 73000/360270 [00:16<01:07, 4248.12 examples/s]Map:  21%|██        | 74000/360270 [00:16<01:04, 4426.24 examples/s]Map:  21%|██        | 75000/360270 [00:16<01:06, 4266.23 examples/s]Map:  21%|██        | 76000/360270 [00:17<01:06, 4298.67 examples/s]Map:  21%|██▏       | 77000/360270 [00:17<01:05, 4350.99 examples/s]Map:  22%|██▏       | 78000/360270 [00:17<01:07, 4194.21 examples/s]Map:  22%|██▏       | 79000/360270 [00:17<01:06, 4222.64 examples/s]Map:  22%|██▏       | 80000/360270 [00:17<01:06, 4240.61 examples/s]Map:  22%|██▏       | 81000/360270 [00:18<01:06, 4190.49 examples/s]Map:  23%|██▎       | 82000/360270 [00:18<01:07, 4135.81 examples/s]Map:  23%|██▎       | 83000/360270 [00:18<01:02, 4407.38 examples/s]Map:  23%|██▎       | 84000/360270 [00:18<01:05, 4203.37 examples/s]Map:  24%|██▎       | 85000/360270 [00:19<01:07, 4082.88 examples/s]Map:  24%|██▍       | 86000/360270 [00:19<01:08, 3996.24 examples/s]Map:  24%|██▍       | 87000/360270 [00:19<01:09, 3919.82 examples/s]Map:  24%|██▍       | 88000/360270 [00:19<01:06, 4098.98 examples/s]Map:  25%|██▍       | 89000/360270 [00:20<01:05, 4130.83 examples/s]Map:  25%|██▍       | 90000/360270 [00:20<01:06, 4091.14 examples/s]Map:  25%|██▌       | 91000/360270 [00:20<01:05, 4135.42 examples/s]Map:  26%|██▌       | 92000/360270 [00:20<01:01, 4387.89 examples/s]Map:  26%|██▌       | 93000/360270 [00:21<01:02, 4265.86 examples/s]Map:  26%|██▌       | 94000/360270 [00:21<01:18, 3385.46 examples/s]Map:  26%|██▋       | 95000/360270 [00:21<01:11, 3698.43 examples/s]Map:  27%|██▋       | 96000/360270 [00:21<01:09, 3824.89 examples/s]Map:  27%|██▋       | 97000/360270 [00:22<01:03, 4123.57 examples/s]Map:  27%|██▋       | 98000/360270 [00:22<00:59, 4378.25 examples/s]Map:  27%|██▋       | 99000/360270 [00:22<00:58, 4479.71 examples/s]Map:  28%|██▊       | 100000/360270 [00:22<01:00, 4291.40 examples/s]Map:  28%|██▊       | 101000/360270 [00:23<01:00, 4294.53 examples/s]Map:  28%|██▊       | 102000/360270 [00:23<01:24, 3059.73 examples/s]Map:  29%|██▊       | 103000/360270 [00:23<01:14, 3439.09 examples/s]Map:  29%|██▉       | 104000/360270 [00:24<01:07, 3812.24 examples/s]Map:  29%|██▉       | 105000/360270 [00:24<01:02, 4062.26 examples/s]Map:  29%|██▉       | 106000/360270 [00:24<01:04, 3935.27 examples/s]Map:  30%|██▉       | 107000/360270 [00:24<00:59, 4221.39 examples/s]Map:  30%|██▉       | 108000/360270 [00:24<00:59, 4245.22 examples/s]Map:  30%|███       | 109000/360270 [00:25<01:01, 4100.17 examples/s]Map:  31%|███       | 110000/360270 [00:25<01:04, 3900.48 examples/s]Map:  31%|███       | 111000/360270 [00:25<01:02, 3989.10 examples/s]Map:  31%|███       | 112000/360270 [00:25<01:00, 4113.67 examples/s]Map:  31%|███▏      | 113000/360270 [00:26<00:58, 4242.28 examples/s]Map:  32%|███▏      | 114000/360270 [00:26<00:58, 4219.33 examples/s]Map:  32%|███▏      | 115000/360270 [00:26<00:58, 4225.51 examples/s]Map:  32%|███▏      | 116000/360270 [00:26<00:56, 4322.78 examples/s]Map:  32%|███▏      | 117000/360270 [00:27<00:57, 4196.53 examples/s]Map:  33%|███▎      | 118000/360270 [00:27<00:56, 4305.73 examples/s]Map:  33%|███▎      | 119000/360270 [00:27<00:54, 4463.70 examples/s]Map:  33%|███▎      | 120000/360270 [00:27<00:53, 4449.50 examples/s]Map:  34%|███▎      | 121000/360270 [00:27<00:53, 4504.21 examples/s]Map:  34%|███▍      | 122000/360270 [00:28<00:52, 4556.35 examples/s]Map:  34%|███▍      | 123000/360270 [00:28<00:55, 4258.16 examples/s]Map:  34%|███▍      | 124000/360270 [00:28<00:54, 4322.50 examples/s]Map:  35%|███▍      | 125000/360270 [00:28<00:53, 4416.30 examples/s]Map:  35%|███▍      | 126000/360270 [00:29<00:53, 4356.57 examples/s]Map:  35%|███▌      | 127000/360270 [00:29<00:50, 4575.11 examples/s]Map:  36%|███▌      | 128000/360270 [00:29<00:47, 4842.96 examples/s]Map:  36%|███▌      | 129000/360270 [00:29<00:50, 4601.92 examples/s]Map:  36%|███▌      | 130000/360270 [00:30<00:53, 4282.01 examples/s]Map:  36%|███▋      | 131000/360270 [00:30<00:51, 4430.66 examples/s]Map:  37%|███▋      | 132000/360270 [00:30<00:51, 4425.72 examples/s]Map:  37%|███▋      | 133000/360270 [00:30<00:49, 4547.84 examples/s]Map:  37%|███▋      | 134000/360270 [00:30<00:51, 4399.40 examples/s]Map:  37%|███▋      | 135000/360270 [00:31<00:52, 4301.53 examples/s]Map:  38%|███▊      | 136000/360270 [00:31<00:55, 4027.06 examples/s]Map:  38%|███▊      | 137000/360270 [00:31<00:54, 4093.19 examples/s]Map:  38%|███▊      | 138000/360270 [00:32<01:32, 2406.47 examples/s]Map:  39%|███▊      | 139000/360270 [00:32<01:20, 2754.61 examples/s]Map:  39%|███▉      | 140000/360270 [00:32<01:10, 3105.07 examples/s]Map:  39%|███▉      | 141000/360270 [00:33<01:04, 3388.37 examples/s]Map:  39%|███▉      | 142000/360270 [00:33<00:59, 3683.41 examples/s]Map:  40%|███▉      | 143000/360270 [00:33<00:53, 4068.94 examples/s]Map:  40%|███▉      | 144000/360270 [00:33<00:50, 4311.62 examples/s]Map:  40%|████      | 145000/360270 [00:33<00:47, 4524.77 examples/s]Map:  41%|████      | 146000/360270 [00:34<00:46, 4571.90 examples/s]Map:  41%|████      | 147000/360270 [00:34<00:48, 4381.81 examples/s]Map:  41%|████      | 148000/360270 [00:34<00:51, 4144.33 examples/s]Map:  41%|████▏     | 149000/360270 [00:34<00:52, 4042.21 examples/s]Map:  42%|████▏     | 150000/360270 [00:35<00:51, 4085.87 examples/s]Map:  42%|████▏     | 151000/360270 [00:35<00:49, 4228.66 examples/s]Map:  42%|████▏     | 152000/360270 [00:35<00:47, 4427.57 examples/s]Map:  42%|████▏     | 153000/360270 [00:35<00:45, 4593.23 examples/s]Map:  43%|████▎     | 154000/360270 [00:36<00:45, 4573.89 examples/s]Map:  43%|████▎     | 155000/360270 [00:36<00:44, 4614.08 examples/s]Map:  43%|████▎     | 156000/360270 [00:36<00:43, 4679.77 examples/s]Map:  44%|████▎     | 157000/360270 [00:36<00:42, 4731.14 examples/s]Map:  44%|████▍     | 158000/360270 [00:36<00:43, 4600.47 examples/s]Map:  44%|████▍     | 159000/360270 [00:37<00:46, 4317.43 examples/s]Map:  44%|████▍     | 160000/360270 [00:37<00:46, 4317.98 examples/s]Map:  45%|████▍     | 161000/360270 [00:37<00:45, 4344.88 examples/s]Map:  45%|████▍     | 162000/360270 [00:37<00:46, 4228.92 examples/s]Map:  45%|████▌     | 163000/360270 [00:38<00:46, 4255.90 examples/s]Map:  46%|████▌     | 164000/360270 [00:38<00:45, 4278.81 examples/s]Map:  46%|████▌     | 165000/360270 [00:38<00:46, 4244.90 examples/s]Map:  46%|████▌     | 166000/360270 [00:38<00:46, 4189.37 examples/s]Map:  46%|████▋     | 167000/360270 [00:39<00:45, 4241.50 examples/s]Map:  47%|████▋     | 168000/360270 [00:39<00:44, 4306.80 examples/s]Map:  47%|████▋     | 169000/360270 [00:39<00:43, 4414.74 examples/s]Map:  47%|████▋     | 170000/360270 [00:39<00:42, 4519.52 examples/s]Map:  47%|████▋     | 171000/360270 [00:39<00:41, 4597.36 examples/s]Map:  48%|████▊     | 172000/360270 [00:40<00:41, 4542.73 examples/s]Map:  48%|████▊     | 173000/360270 [00:40<00:44, 4213.67 examples/s]Map:  48%|████▊     | 174000/360270 [00:41<01:06, 2799.65 examples/s]Map:  49%|████▊     | 175000/360270 [00:41<01:02, 2963.29 examples/s]Map:  49%|████▉     | 176000/360270 [00:41<00:58, 3139.87 examples/s]Map:  49%|████▉     | 177000/360270 [00:41<00:54, 3367.98 examples/s]Map:  49%|████▉     | 178000/360270 [00:42<00:51, 3532.15 examples/s]Map:  50%|████▉     | 179000/360270 [00:42<00:48, 3763.54 examples/s]Map:  50%|████▉     | 180000/360270 [00:42<00:45, 3966.74 examples/s]Map:  50%|█████     | 181000/360270 [00:42<00:43, 4121.43 examples/s]Map:  51%|█████     | 182000/360270 [00:43<00:42, 4241.06 examples/s]Map:  51%|█████     | 183000/360270 [00:43<00:43, 4068.55 examples/s]Map:  51%|█████     | 184000/360270 [00:43<00:41, 4286.26 examples/s]Map:  51%|█████▏    | 185000/360270 [00:43<00:40, 4290.14 examples/s]Map:  52%|█████▏    | 186000/360270 [00:43<00:39, 4365.88 examples/s]Map:  52%|█████▏    | 187000/360270 [00:44<00:39, 4405.92 examples/s]Map:  52%|█████▏    | 188000/360270 [00:44<01:03, 2692.82 examples/s]Map:  52%|█████▏    | 189000/360270 [00:45<00:56, 3052.28 examples/s]Map:  53%|█████▎    | 190000/360270 [00:45<00:51, 3328.23 examples/s]Map:  53%|█████▎    | 191000/360270 [00:45<00:46, 3640.23 examples/s]Map:  53%|█████▎    | 192000/360270 [00:45<00:44, 3791.05 examples/s]Map:  54%|█████▎    | 193000/360270 [00:46<00:41, 4010.01 examples/s]Map:  54%|█████▍    | 194000/360270 [00:46<00:39, 4253.83 examples/s]Map:  54%|█████▍    | 195000/360270 [00:46<00:37, 4401.92 examples/s]Map:  54%|█████▍    | 196000/360270 [00:46<00:37, 4432.41 examples/s]Map:  55%|█████▍    | 197000/360270 [00:46<00:35, 4595.13 examples/s]Map:  55%|█████▍    | 198000/360270 [00:47<00:35, 4555.37 examples/s]Map:  55%|█████▌    | 199000/360270 [00:47<00:34, 4638.02 examples/s]Map:  56%|█████▌    | 200000/360270 [00:47<00:34, 4708.52 examples/s]Map:  56%|█████▌    | 201000/360270 [00:47<00:33, 4760.61 examples/s]Map:  56%|█████▌    | 202000/360270 [00:47<00:33, 4681.65 examples/s]Map:  56%|█████▋    | 203000/360270 [00:48<00:33, 4746.70 examples/s]Map:  57%|█████▋    | 204000/360270 [00:48<00:32, 4808.12 examples/s]Map:  57%|█████▋    | 205000/360270 [00:48<00:33, 4609.95 examples/s]Map:  57%|█████▋    | 206000/360270 [00:48<00:33, 4568.71 examples/s]Map:  57%|█████▋    | 207000/360270 [00:48<00:32, 4688.79 examples/s]Map:  58%|█████▊    | 208000/360270 [00:49<00:33, 4579.19 examples/s]Map:  58%|█████▊    | 209000/360270 [00:49<00:33, 4578.94 examples/s]Map:  58%|█████▊    | 210000/360270 [00:50<00:51, 2940.34 examples/s]Map:  59%|█████▊    | 211000/360270 [00:50<00:46, 3187.69 examples/s]Map:  59%|█████▉    | 212000/360270 [00:50<00:42, 3460.13 examples/s]Map:  59%|█████▉    | 213000/360270 [00:50<00:39, 3768.38 examples/s]Map:  59%|█████▉    | 214000/360270 [00:50<00:37, 3869.92 examples/s]Map:  60%|█████▉    | 215000/360270 [00:51<00:36, 3933.56 examples/s]Map:  60%|█████▉    | 216000/360270 [00:51<00:35, 4030.39 examples/s]Map:  60%|██████    | 217000/360270 [00:51<00:33, 4266.83 examples/s]Map:  61%|██████    | 218000/360270 [00:51<00:31, 4492.97 examples/s]Map:  61%|██████    | 219000/360270 [00:52<00:31, 4483.97 examples/s]Map:  61%|██████    | 220000/360270 [00:52<00:30, 4619.48 examples/s]Map:  61%|██████▏   | 221000/360270 [00:52<00:29, 4800.71 examples/s]Map:  62%|██████▏   | 222000/360270 [00:52<00:29, 4660.74 examples/s]Map:  62%|██████▏   | 223000/360270 [00:52<00:29, 4667.69 examples/s]Map:  62%|██████▏   | 224000/360270 [00:53<00:30, 4417.41 examples/s]Map:  62%|██████▏   | 225000/360270 [00:53<00:31, 4325.76 examples/s]Map:  63%|██████▎   | 226000/360270 [00:53<00:31, 4269.84 examples/s]Map:  63%|██████▎   | 227000/360270 [00:53<00:30, 4423.76 examples/s]Map:  63%|██████▎   | 228000/360270 [00:54<00:30, 4406.47 examples/s]Map:  64%|██████▎   | 229000/360270 [00:54<00:31, 4205.69 examples/s]Map:  64%|██████▍   | 230000/360270 [00:54<00:29, 4353.54 examples/s]Map:  64%|██████▍   | 231000/360270 [00:54<00:29, 4325.15 examples/s]Map:  64%|██████▍   | 232000/360270 [00:55<00:31, 4042.98 examples/s]Map:  65%|██████▍   | 233000/360270 [00:55<00:30, 4187.45 examples/s]Map:  65%|██████▍   | 234000/360270 [00:55<00:29, 4290.60 examples/s]Map:  65%|██████▌   | 235000/360270 [00:55<00:29, 4189.49 examples/s]Map:  66%|██████▌   | 236000/360270 [00:55<00:28, 4427.06 examples/s]Map:  66%|██████▌   | 237000/360270 [00:56<00:27, 4440.21 examples/s]Map:  66%|██████▌   | 238000/360270 [00:56<00:27, 4435.65 examples/s]Map:  66%|██████▋   | 239000/360270 [00:56<00:26, 4534.18 examples/s]Map:  67%|██████▋   | 240000/360270 [00:56<00:26, 4492.51 examples/s]Map:  67%|██████▋   | 241000/360270 [00:57<00:27, 4341.54 examples/s]Map:  67%|██████▋   | 242000/360270 [00:57<00:26, 4394.60 examples/s]Map:  67%|██████▋   | 243000/360270 [00:57<00:28, 4120.68 examples/s]Map:  68%|██████▊   | 244000/360270 [00:57<00:29, 3997.94 examples/s]Map:  68%|██████▊   | 245000/360270 [00:58<00:28, 4034.67 examples/s]Map:  68%|██████▊   | 246000/360270 [00:58<00:45, 2538.70 examples/s]Map:  69%|██████▊   | 247000/360270 [00:59<00:39, 2898.44 examples/s]Map:  69%|██████▉   | 248000/360270 [00:59<00:36, 3044.41 examples/s]Map:  69%|██████▉   | 249000/360270 [00:59<00:33, 3308.81 examples/s]Map:  69%|██████▉   | 250000/360270 [00:59<00:30, 3641.05 examples/s]Map:  70%|██████▉   | 251000/360270 [01:00<00:29, 3709.71 examples/s]Map:  70%|██████▉   | 252000/360270 [01:00<00:29, 3620.83 examples/s]Map:  70%|███████   | 253000/360270 [01:00<00:29, 3612.36 examples/s]Map:  71%|███████   | 254000/360270 [01:00<00:29, 3610.35 examples/s]Map:  71%|███████   | 255000/360270 [01:01<00:29, 3610.39 examples/s]Map:  71%|███████   | 256000/360270 [01:01<00:29, 3516.14 examples/s]Map:  71%|███████▏  | 257000/360270 [01:01<00:28, 3663.99 examples/s]Map:  72%|███████▏  | 258000/360270 [01:02<00:27, 3676.22 examples/s]Map:  72%|███████▏  | 259000/360270 [01:02<00:28, 3530.20 examples/s]Map:  72%|███████▏  | 260000/360270 [01:02<00:28, 3491.24 examples/s]Map:  72%|███████▏  | 261000/360270 [01:02<00:28, 3513.41 examples/s]Map:  73%|███████▎  | 262000/360270 [01:03<00:28, 3461.57 examples/s]Map:  73%|███████▎  | 263000/360270 [01:03<00:24, 3891.10 examples/s]Map:  73%|███████▎  | 264000/360270 [01:03<00:23, 4075.07 examples/s]Map:  74%|███████▎  | 265000/360270 [01:03<00:23, 4140.07 examples/s]Map:  74%|███████▍  | 266000/360270 [01:04<00:22, 4144.37 examples/s]Map:  74%|███████▍  | 267000/360270 [01:04<00:22, 4091.36 examples/s]Map:  74%|███████▍  | 268000/360270 [01:04<00:22, 4157.39 examples/s]Map:  75%|███████▍  | 269000/360270 [01:04<00:21, 4232.30 examples/s]Map:  75%|███████▍  | 270000/360270 [01:05<00:22, 4084.80 examples/s]Map:  75%|███████▌  | 271000/360270 [01:05<00:21, 4140.19 examples/s]Map:  75%|███████▌  | 272000/360270 [01:05<00:21, 4126.30 examples/s]Map:  76%|███████▌  | 273000/360270 [01:05<00:21, 4055.98 examples/s]Map:  76%|███████▌  | 274000/360270 [01:06<00:21, 3996.93 examples/s]Map:  76%|███████▋  | 275000/360270 [01:06<00:21, 4056.63 examples/s]Map:  77%|███████▋  | 276000/360270 [01:06<00:20, 4201.08 examples/s]Map:  77%|███████▋  | 277000/360270 [01:06<00:18, 4390.00 examples/s]Map:  77%|███████▋  | 278000/360270 [01:06<00:19, 4295.37 examples/s]Map:  77%|███████▋  | 279000/360270 [01:07<00:18, 4305.70 examples/s]Map:  78%|███████▊  | 280000/360270 [01:07<00:18, 4327.65 examples/s]Map:  78%|███████▊  | 281000/360270 [01:07<00:18, 4327.03 examples/s]Map:  78%|███████▊  | 282000/360270 [01:08<00:26, 2950.81 examples/s]Map:  79%|███████▊  | 283000/360270 [01:08<00:22, 3368.39 examples/s]Map:  79%|███████▉  | 284000/360270 [01:08<00:21, 3619.55 examples/s]Map:  79%|███████▉  | 285000/360270 [01:08<00:20, 3684.25 examples/s]Map:  79%|███████▉  | 286000/360270 [01:09<00:19, 3788.36 examples/s]Map:  80%|███████▉  | 287000/360270 [01:09<00:18, 3969.81 examples/s]Map:  80%|███████▉  | 288000/360270 [01:09<00:17, 4095.81 examples/s]Map:  80%|████████  | 289000/360270 [01:09<00:17, 4068.36 examples/s]Map:  80%|████████  | 290000/360270 [01:10<00:16, 4192.79 examples/s]Map:  81%|████████  | 291000/360270 [01:10<00:16, 4211.86 examples/s]Map:  81%|████████  | 292000/360270 [01:10<00:16, 4186.72 examples/s]Map:  81%|████████▏ | 293000/360270 [01:10<00:15, 4251.89 examples/s]Map:  82%|████████▏ | 294000/360270 [01:11<00:15, 4221.08 examples/s]Map:  82%|████████▏ | 295000/360270 [01:11<00:15, 4179.61 examples/s]Map:  82%|████████▏ | 296000/360270 [01:11<00:15, 4102.27 examples/s]Map:  82%|████████▏ | 297000/360270 [01:11<00:15, 4173.52 examples/s]Map:  83%|████████▎ | 298000/360270 [01:11<00:14, 4289.05 examples/s]Map:  83%|████████▎ | 299000/360270 [01:12<00:15, 4049.41 examples/s]Map:  83%|████████▎ | 300000/360270 [01:12<00:15, 3837.61 examples/s]Map:  84%|████████▎ | 301000/360270 [01:12<00:15, 3892.91 examples/s]Map:  84%|████████▍ | 302000/360270 [01:13<00:14, 3929.97 examples/s]Map:  84%|████████▍ | 303000/360270 [01:13<00:14, 4011.30 examples/s]Map:  84%|████████▍ | 304000/360270 [01:13<00:13, 4084.48 examples/s]Map:  85%|████████▍ | 305000/360270 [01:13<00:13, 4197.95 examples/s]Map:  85%|████████▍ | 306000/360270 [01:13<00:12, 4193.12 examples/s]Map:  85%|████████▌ | 307000/360270 [01:14<00:12, 4325.92 examples/s]Map:  85%|████████▌ | 308000/360270 [01:14<00:11, 4398.76 examples/s]Map:  86%|████████▌ | 309000/360270 [01:14<00:11, 4396.05 examples/s]Map:  86%|████████▌ | 310000/360270 [01:14<00:11, 4431.67 examples/s]Map:  86%|████████▋ | 311000/360270 [01:15<00:10, 4559.78 examples/s]Map:  87%|████████▋ | 312000/360270 [01:15<00:10, 4488.69 examples/s]Map:  87%|████████▋ | 313000/360270 [01:15<00:10, 4464.14 examples/s]Map:  87%|████████▋ | 314000/360270 [01:15<00:09, 4651.98 examples/s]Map:  87%|████████▋ | 315000/360270 [01:15<00:09, 4614.54 examples/s]Map:  88%|████████▊ | 316000/360270 [01:16<00:10, 4379.94 examples/s]Map:  88%|████████▊ | 317000/360270 [01:16<00:09, 4545.39 examples/s]Map:  88%|████████▊ | 318000/360270 [01:16<00:14, 2983.34 examples/s]Map:  89%|████████▊ | 319000/360270 [01:17<00:12, 3352.95 examples/s]Map:  89%|████████▉ | 320000/360270 [01:17<00:10, 3772.55 examples/s]Map:  89%|████████▉ | 321000/360270 [01:17<00:10, 3910.72 examples/s]Map:  89%|████████▉ | 322000/360270 [01:17<00:09, 3873.32 examples/s]Map:  90%|████████▉ | 323000/360270 [01:18<00:09, 4017.05 examples/s]Map:  90%|████████▉ | 324000/360270 [01:18<00:09, 4000.20 examples/s]Map:  90%|█████████ | 325000/360270 [01:18<00:08, 4040.38 examples/s]Map:  90%|█████████ | 326000/360270 [01:18<00:08, 4192.59 examples/s]Map:  91%|█████████ | 327000/360270 [01:19<00:07, 4249.42 examples/s]Map:  91%|█████████ | 328000/360270 [01:19<00:07, 4120.59 examples/s]Map:  91%|█████████▏| 329000/360270 [01:19<00:07, 4084.07 examples/s]Map:  92%|█████████▏| 330000/360270 [01:19<00:07, 4070.54 examples/s]Map:  92%|█████████▏| 331000/360270 [01:20<00:07, 4052.30 examples/s]Map:  92%|█████████▏| 332000/360270 [01:20<00:06, 4162.62 examples/s]Map:  92%|█████████▏| 333000/360270 [01:20<00:06, 4214.63 examples/s]Map:  93%|█████████▎| 334000/360270 [01:20<00:06, 4180.84 examples/s]Map:  93%|█████████▎| 335000/360270 [01:20<00:06, 4198.59 examples/s]Map:  93%|█████████▎| 336000/360270 [01:21<00:05, 4123.43 examples/s]Map:  94%|█████████▎| 337000/360270 [01:21<00:05, 4362.01 examples/s]Map:  94%|█████████▍| 338000/360270 [01:21<00:04, 4545.95 examples/s]Map:  94%|█████████▍| 339000/360270 [01:21<00:04, 4294.39 examples/s]Map:  94%|█████████▍| 340000/360270 [01:22<00:04, 4348.78 examples/s]Map:  95%|█████████▍| 341000/360270 [01:22<00:04, 4372.52 examples/s]Map:  95%|█████████▍| 342000/360270 [01:22<00:04, 4172.07 examples/s]Map:  95%|█████████▌| 343000/360270 [01:22<00:04, 4204.43 examples/s]Map:  95%|█████████▌| 344000/360270 [01:23<00:04, 4062.78 examples/s]Map:  96%|█████████▌| 345000/360270 [01:23<00:03, 4050.35 examples/s]Map:  96%|█████████▌| 346000/360270 [01:23<00:03, 4124.29 examples/s]Map:  96%|█████████▋| 347000/360270 [01:23<00:03, 4327.98 examples/s]Map:  97%|█████████▋| 348000/360270 [01:24<00:02, 4216.40 examples/s]Map:  97%|█████████▋| 349000/360270 [01:24<00:02, 4207.34 examples/s]Map:  97%|█████████▋| 350000/360270 [01:24<00:02, 4192.15 examples/s]Map:  97%|█████████▋| 351000/360270 [01:24<00:02, 4017.11 examples/s]Map:  98%|█████████▊| 352000/360270 [01:25<00:01, 4239.96 examples/s]Map:  98%|█████████▊| 353000/360270 [01:25<00:01, 4408.27 examples/s]Map:  98%|█████████▊| 354000/360270 [01:25<00:02, 3067.36 examples/s]Map:  99%|█████████▊| 355000/360270 [01:26<00:01, 3296.82 examples/s]Map:  99%|█████████▉| 356000/360270 [01:26<00:01, 3655.93 examples/s][2025-06-17 03:56:23,227] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
Map:  99%|█████████▉| 357000/360270 [01:26<00:00, 3907.43 examples/s][2025-06-17 03:56:23,488] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
Map:  99%|█████████▉| 358000/360270 [01:26<00:00, 4224.66 examples/s]Map: 100%|█████████▉| 359000/360270 [01:26<00:00, 4360.23 examples/s]Map: 100%|█████████▉| 360000/360270 [01:27<00:00, 4448.74 examples/s]Map: 100%|██████████| 360270/360270 [01:27<00:00, 4134.19 examples/s]
Tokenization complete. Dataset ready with 360270 examples.
[2025-06-17 03:56:24,049] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.17.1+e440506b, git-hash=e440506b, git-branch=master
[2025-06-17 03:56:24,049] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 03:56:25,606] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 03:56:31,717] [INFO] [engine.py:1325:_configure_distributed_model] ********** distributed groups summary **********
	 self.dp_world_size=8
	 self.mp_world_size=1
	 self.seq_dp_world_size=8
	 self.sequence_parallel_size=1
***********************************************
NCCL version 2.26.2+cuda12.2
[2025-06-17 03:56:33,374] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 03:56:34,459] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 03:56:34,671] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 03:56:34,964] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 03:56:45,999] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-06-17 03:56:45,999] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-06-17 03:56:45,999] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-06-17 03:56:46,005] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-06-17 03:56:46,005] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2025-06-17 03:56:46,005] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 1 optimizer
[2025-06-17 03:56:46,005] [INFO] [stage_1_and_2.py:151:__init__] Reduce bucket size 500000000
[2025-06-17 03:56:46,005] [INFO] [stage_1_and_2.py:152:__init__] Allgather bucket size 500000000
[2025-06-17 03:56:46,005] [INFO] [stage_1_and_2.py:153:__init__] CPU Offload: False
[2025-06-17 03:56:46,005] [INFO] [stage_1_and_2.py:154:__init__] Round robin gradient partitioning: False
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'> optimizer: <class 'accelerate.utils.deepspeed.DeepSpeedOptimizerWrapper'>
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'> optimizer: <class 'accelerate.utils.deepspeed.DeepSpeedOptimizerWrapper'>
[2025-06-17 03:57:07,909] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'> optimizer: <class 'accelerate.utils.deepspeed.DeepSpeedOptimizerWrapper'>
[2025-06-17 03:57:07,910] [INFO] [utils.py:782:see_memory_usage] MA 18.7 GB         Max_MA 20.57 GB         CA 20.57 GB         Max_CA 21 GB 
[2025-06-17 03:57:07,910] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 179.42 GB, percent = 9.9%
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[2025-06-17 03:57:08,082] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-06-17 03:57:08,083] [INFO] [utils.py:782:see_memory_usage] MA 18.7 GB         Max_MA 22.44 GB         CA 24.31 GB         Max_CA 24 GB 
[2025-06-17 03:57:08,083] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 180.22 GB, percent = 9.9%
[2025-06-17 03:57:08,083] [INFO] [stage_1_and_2.py:573:__init__] optimizer state initialized
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[2025-06-17 03:57:08,244] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-06-17 03:57:08,245] [INFO] [utils.py:782:see_memory_usage] MA 18.7 GB         Max_MA 18.7 GB         CA 24.31 GB         Max_CA 24 GB 
[2025-06-17 03:57:08,245] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 181.02 GB, percent = 9.9%
[2025-06-17 03:57:08,246] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-06-17 03:57:08,246] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-06-17 03:57:08,246] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-06-17 03:57:08,246] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-06], mom=[(0.9, 0.999)]
[2025-06-17 03:57:08,247] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True
[2025-06-17 03:57:08,247] [INFO] [config.py:921:print] DeepSpeedEngine configuration:
[2025-06-17 03:57:08,247] [INFO] [config.py:925:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-06-17 03:57:08,247] [INFO] [config.py:925:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-06-17 03:57:08,247] [INFO] [config.py:925:print]   amp_enabled .................. False
[2025-06-17 03:57:08,247] [INFO] [config.py:925:print]   amp_params ................... False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   bfloat16_config .............. enabled=True immediate_grad_update=False check_grad_overflow=False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   checkpoint_parallel_write_pipeline  False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   checkpoint_tag_validation_enabled  True
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   checkpoint_tag_validation_fail  False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x73d4841d73d0>
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   communication_data_type ...... None
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   curriculum_enabled_legacy .... False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   curriculum_params_legacy ..... False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   data_efficiency_enabled ...... False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   dataloader_drop_last ......... False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   disable_allgather ............ False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   dump_state ................... False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   eigenvalue_enabled ........... False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   eigenvalue_gas_boundary_resolution  1
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   eigenvalue_layer_num ......... 0
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   eigenvalue_max_iter .......... 100
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   eigenvalue_stability ......... 1e-06
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   eigenvalue_tol ............... 0.01
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   eigenvalue_verbose ........... False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   elasticity_enabled ........... False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   float16_config ............... enabled=False auto_cast=False loss_scale=0.0 initial_scale_power=16 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   global_rank .................. 0
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   grad_accum_dtype ............. None
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   gradient_accumulation_steps .. 4
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   gradient_clipping ............ 1.0
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   gradient_predivide_factor .... 1.0
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   graph_harvesting ............. False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   load_universal_checkpoint .... False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   memory_breakdown ............. False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   mics_hierarchial_params_gather  False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   mics_shard_size .............. -1
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   optimizer_legacy_fusion ...... False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   optimizer_name ............... None
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   optimizer_params ............. None
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   pld_enabled .................. False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   pld_params ................... False
[2025-06-17 03:57:08,248] [INFO] [config.py:925:print]   prescale_gradients ........... False
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   scheduler_name ............... None
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   scheduler_params ............. None
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   seq_parallel_communication_data_type  torch.float32
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   sparse_attention ............. None
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   sparse_gradients_enabled ..... False
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   steps_per_print .............. inf
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   timers_config ................ enabled=True synchronized=True
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   train_batch_size ............. 32
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   train_micro_batch_size_per_gpu  1
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   use_data_before_expert_parallel_  False
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   use_node_local_storage ....... False
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   wall_clock_breakdown ......... False
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   weight_quantization_config ... None
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   world_size ................... 8
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   zero_allow_untested_optimizer  True
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   zero_enabled ................. True
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   zero_force_ds_cpu_optimizer .. True
[2025-06-17 03:57:08,249] [INFO] [config.py:925:print]   zero_optimization_stage ...... 1
[2025-06-17 03:57:08,249] [INFO] [config.py:911:print_user_config]   json = {
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 1, 
        "sub_group_size": 1.000000e+08
    }, 
    "compile": {
        "deepcompile": false, 
        "offload_activation": false, 
        "offload_opt_states": false, 
        "double_buffer": true, 
        "symmetric_memory": false, 
        "free_activation": false, 
        "debug_log": false, 
        "sync_before_reduce": false, 
        "sync_after_reduce": false
    }, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 1, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'> optimizer: <class 'accelerate.utils.deepspeed.DeepSpeedOptimizerWrapper'>
wandb: Currently logged in as: mtanaka (orangewandb) to https://msaip.wandb.io. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.20.1
wandb: Run data is saved locally in /home/mtanaka/work/dc/ds_verify_loss/wandb/run-20250617_035708-mts103b7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Meta-Llama-3-8B_np8ds1Binductorz1L0bs1seq512acc4ac0c0dc0pass_none_os0T20250617035708
wandb: ⭐️ View project at https://msaip.wandb.io/orangewandb/ds-verify-loss
wandb: 🚀 View run at https://msaip.wandb.io/orangewandb/ds-verify-loss/runs/mts103b7
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'> optimizer: <class 'accelerate.utils.deepspeed.DeepSpeedOptimizerWrapper'>
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'> optimizer: <class 'accelerate.utils.deepspeed.DeepSpeedOptimizerWrapper'>
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'> optimizer: <class 'accelerate.utils.deepspeed.DeepSpeedOptimizerWrapper'>
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'> optimizer: <class 'accelerate.utils.deepspeed.DeepSpeedOptimizerWrapper'>
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Epoch 1, Step 40, Loss: 10.555366 sync: True time: 0.4812662601470947 alloc_mem: 28307113472 peak_mem: 50064946688
Epoch 1, Step 80, Loss: 7.431078 sync: True time: 0.47615957260131836 alloc_mem: 28307113472 peak_mem: 50064946688
Epoch 1, Step 120, Loss: 6.583869 sync: True time: 0.4767730236053467 alloc_mem: 28307113472 peak_mem: 50064946688
Epoch 1, Step 160, Loss: 4.481570 sync: True time: 0.47744226455688477 alloc_mem: 28307113472 peak_mem: 50064946688
Epoch 1, Step 200, Loss: 2.041164 sync: True time: 0.4791698455810547 alloc_mem: 28307113472 peak_mem: 50064946688
Epoch 1, Step 240, Loss: 2.750041 sync: True time: 0.4786558151245117 alloc_mem: 28307113472 peak_mem: 50064946688
Epoch 1, Step 280, Loss: 1.206711 sync: True time: 0.47495031356811523 alloc_mem: 28307113472 peak_mem: 50064946688
Epoch 1, Step 320, Loss: 0.363938 sync: True time: 0.47672104835510254 alloc_mem: 28307113472 peak_mem: 50064946688
Epoch 1, Step 360, Loss: 2.245553 sync: True time: 0.47898101806640625 alloc_mem: 28307113472 peak_mem: 50064946688
Epoch 1, Step 400, Loss: 1.620094 sync: True time: 0.4779543876647949 alloc_mem: 28307113472 peak_mem: 50064946688
meta-llama/Meta-Llama-3-8B ds=True np=8 batch_size=1 seq=512 zero_stage=1 acc=4 ac=False compile=False backend=inductor deepcompile=False passes=None compile_time=0 iteration time: 0.4782 alloc_mem: 28307113472 peak_mem: 50064946688
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      summary/average_iteration_time ▁
wandb:            summary/compile_time_sum ▁
wandb: summary/final_cuda_memory_allocated ▁
wandb:      summary/final_cuda_memory_peak ▁
wandb:                 summary/total_steps ▁
wandb:        system/cuda_memory_allocated ▁▁▁▁▁▁▁▁▁▁
wandb:             system/cuda_memory_peak ▁▁▁▁▁▁▁▁▁▁
wandb:               timing/iteration_time █▂▃▄▅▅▁▃▅▄
wandb:                         train/epoch ▁▁▁▁▁▁▁▁▁▁
wandb:                   train/global_step ▁▂▃▃▄▅▆▆▇█
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▆▅▄▂▃▂▁▂▂
wandb: 
wandb: Run summary:
wandb:      summary/average_iteration_time 0.47825
wandb:            summary/compile_time_sum 0
wandb: summary/final_cuda_memory_allocated 28307113472
wandb:      summary/final_cuda_memory_peak 50064946688
wandb:                 summary/total_steps 400
wandb:        system/cuda_memory_allocated 28307113472
wandb:             system/cuda_memory_peak 50064946688
wandb:               timing/iteration_time 0.47841
wandb:                         train/epoch 1
wandb:                   train/global_step 400
wandb:                 train/learning_rate 0.0
wandb:                          train/loss 1.62009
wandb: 
wandb: 🚀 View run Meta-Llama-3-8B_np8ds1Binductorz1L0bs1seq512acc4ac0c0dc0pass_none_os0T20250617035708 at: https://msaip.wandb.io/orangewandb/ds-verify-loss/runs/mts103b7
wandb: ⭐️ View project at: https://msaip.wandb.io/orangewandb/ds-verify-loss
wandb: Synced 8 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250617_035708-mts103b7/logs
[rank0]:[W617 03:58:15.347176580 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
