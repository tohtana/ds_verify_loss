NUM_NODES: 1 NGPUS_PER_NODE: 8 NUM_PROCESSES: 8
HOST_IP: 127.0.0.1
NUM_NODES: 1
NUM_PROCESSES: 8
BACKEND: deepspeed
ZERO_STAGE: 3
MODEL: meta-llama/Meta-Llama-3-8B
GRADIENT_ACCUMULATION_STEPS: 4
EXTRA_OPTS:  --gradient_accumulation_steps 4 --dataset_percentage 20.0 --compile --use_wandb
Logging to logs/debug_n0_Meta-Llama-3-8B_deepspeed_np8z3c1dc1E0b1seq512g4a1pALL.log
[2025-06-17 04:25:48,181] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 04:25:48,259] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 04:25:49,793] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
W0617 04:25:50.279000 374191 site-packages/torch/distributed/run.py:766] 
W0617 04:25:50.279000 374191 site-packages/torch/distributed/run.py:766] *****************************************
W0617 04:25:50.279000 374191 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0617 04:25:50.279000 374191 site-packages/torch/distributed/run.py:766] *****************************************
[W617 04:25:50.993412166 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W617 04:25:50.993435766 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [fdff:ffff:6a7a::da5c:0]:34817 (errno: 97 - Address family not supported by protocol).
Namespace(model_name='meta-llama/Meta-Llama-3-8B', batch_size=1, num_epochs=5, seq_length=512, learning_rate=1e-06, max_grad_norm=1.0, gradient_accumulation_steps=4, activation_checkpointing=False, eval=False, dataset_name='wikitext', dataset_percentage=20.0, num_layers=0, attn_impl='sdpa', compile=True, passes=None, backend='inductor', offload_opt_states=False, profile=False, deterministic=False, profile_dir=None, bench_step=100, warmup_step=15, zero_stage=3, log_interval=10, save_weights=False, load_weights=False, use_wandb=True, wandb_project='ds-verify-loss', wandb_run_name=None, wandb_tags=[])
Namespace(model_name='meta-llama/Meta-Llama-3-8B', batch_size=1, num_epochs=5, seq_length=512, learning_rate=1e-06, max_grad_norm=1.0, gradient_accumulation_steps=4, activation_checkpointing=False, eval=False, dataset_name='wikitext', dataset_percentage=20.0, num_layers=0, attn_impl='sdpa', compile=True, passes=None, backend='inductor', offload_opt_states=False, profile=False, deterministic=False, profile_dir=None, bench_step=100, warmup_step=15, zero_stage=3, log_interval=10, save_weights=False, load_weights=False, use_wandb=True, wandb_project='ds-verify-loss', wandb_run_name=None, wandb_tags=[])
Namespace(model_name='meta-llama/Meta-Llama-3-8B', batch_size=1, num_epochs=5, seq_length=512, learning_rate=1e-06, max_grad_norm=1.0, gradient_accumulation_steps=4, activation_checkpointing=False, eval=False, dataset_name='wikitext', dataset_percentage=20.0, num_layers=0, attn_impl='sdpa', compile=True, passes=None, backend='inductor', offload_opt_states=False, profile=False, deterministic=False, profile_dir=None, bench_step=100, warmup_step=15, zero_stage=3, log_interval=10, save_weights=False, load_weights=False, use_wandb=True, wandb_project='ds-verify-loss', wandb_run_name=None, wandb_tags=[])
Namespace(model_name='meta-llama/Meta-Llama-3-8B', batch_size=1, num_epochs=5, seq_length=512, learning_rate=1e-06, max_grad_norm=1.0, gradient_accumulation_steps=4, activation_checkpointing=False, eval=False, dataset_name='wikitext', dataset_percentage=20.0, num_layers=0, attn_impl='sdpa', compile=True, passes=None, backend='inductor', offload_opt_states=False, profile=False, deterministic=False, profile_dir=None, bench_step=100, warmup_step=15, zero_stage=3, log_interval=10, save_weights=False, load_weights=False, use_wandb=True, wandb_project='ds-verify-loss', wandb_run_name=None, wandb_tags=[])
Namespace(model_name='meta-llama/Meta-Llama-3-8B', batch_size=1, num_epochs=5, seq_length=512, learning_rate=1e-06, max_grad_norm=1.0, gradient_accumulation_steps=4, activation_checkpointing=False, eval=False, dataset_name='wikitext', dataset_percentage=20.0, num_layers=0, attn_impl='sdpa', compile=True, passes=None, backend='inductor', offload_opt_states=False, profile=False, deterministic=False, profile_dir=None, bench_step=100, warmup_step=15, zero_stage=3, log_interval=10, save_weights=False, load_weights=False, use_wandb=True, wandb_project='ds-verify-loss', wandb_run_name=None, wandb_tags=[])
Namespace(model_name='meta-llama/Meta-Llama-3-8B', batch_size=1, num_epochs=5, seq_length=512, learning_rate=1e-06, max_grad_norm=1.0, gradient_accumulation_steps=4, activation_checkpointing=False, eval=False, dataset_name='wikitext', dataset_percentage=20.0, num_layers=0, attn_impl='sdpa', compile=True, passes=None, backend='inductor', offload_opt_states=False, profile=False, deterministic=False, profile_dir=None, bench_step=100, warmup_step=15, zero_stage=3, log_interval=10, save_weights=False, load_weights=False, use_wandb=True, wandb_project='ds-verify-loss', wandb_run_name=None, wandb_tags=[])
Namespace(model_name='meta-llama/Meta-Llama-3-8B', batch_size=1, num_epochs=5, seq_length=512, learning_rate=1e-06, max_grad_norm=1.0, gradient_accumulation_steps=4, activation_checkpointing=False, eval=False, dataset_name='wikitext', dataset_percentage=20.0, num_layers=0, attn_impl='sdpa', compile=True, passes=None, backend='inductor', offload_opt_states=False, profile=False, deterministic=False, profile_dir=None, bench_step=100, warmup_step=15, zero_stage=3, log_interval=10, save_weights=False, load_weights=False, use_wandb=True, wandb_project='ds-verify-loss', wandb_run_name=None, wandb_tags=[])
Namespace(model_name='meta-llama/Meta-Llama-3-8B', batch_size=1, num_epochs=5, seq_length=512, learning_rate=1e-06, max_grad_norm=1.0, gradient_accumulation_steps=4, activation_checkpointing=False, eval=False, dataset_name='wikitext', dataset_percentage=20.0, num_layers=0, attn_impl='sdpa', compile=True, passes=None, backend='inductor', offload_opt_states=False, profile=False, deterministic=False, profile_dir=None, bench_step=100, warmup_step=15, zero_stage=3, log_interval=10, save_weights=False, load_weights=False, use_wandb=True, wandb_project='ds-verify-loss', wandb_run_name=None, wandb_tags=[])
[2025-06-17 04:25:54,613] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 04:25:55,714] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 04:25:56,197] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 04:25:56,476] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 04:25:56,487] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 04:25:56,489] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 04:25:56,536] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 04:25:56,543] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 04:25:56,552] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 04:25:56,553] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 04:25:56,609] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-17 04:25:56,726] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 04:25:56,727] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 04:25:56,798] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 04:25:56,800] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 04:25:56,804] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 04:25:56,849] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-17 04:25:57,179] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-06-17 04:25:57,179] [INFO] [comm.py:706:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W617 04:25:57.890667817 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W617 04:25:57.890710225 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [fdff:ffff:bc79:0:2c29:3ed3:e57:0]:60520 (errno: 97 - Address family not supported by protocol).
Running on device: cuda:0 is_deepspeed: True
Loading model and tokenizer...
[2025-06-17 04:25:57,529] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-17 04:25:57,555] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 04:25:57,693] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-17 04:25:57,726] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
NCCL version 2.26.2+cuda12.2
[2025-06-17 04:25:57,806] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-17 04:25:57,809] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-17 04:25:57,833] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-17 04:25:57,884] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-17 04:25:58,249] [INFO] [comm.py:675:init_distributed] cdb=None
[W617 04:25:58.960589547 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W617 04:25:58.960626822 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [fdff:ffff:3977:0:2c69:ec00:4e64:0]:18151 (errno: 97 - Address family not supported by protocol).
[2025-06-17 04:25:58,362] [INFO] [comm.py:675:init_distributed] cdb=None
[W617 04:25:58.072956754 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W617 04:25:58.072988927 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [fdff:ffff:a17d:0:2ca9:470a:9e60:0]:18057 (errno: 97 - Address family not supported by protocol).
[2025-06-17 04:25:58,388] [INFO] [comm.py:675:init_distributed] cdb=None
[W617 04:25:58.099329119 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W617 04:25:58.099371062 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [fdff:ffff:4670:0:2cb9:7b20:fc5d:0]:24313 (errno: 97 - Address family not supported by protocol).
[2025-06-17 04:25:58,424] [INFO] [comm.py:675:init_distributed] cdb=None
[W617 04:25:58.135261037 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W617 04:25:58.135299919 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [fdff:ffff:37f:0:2c99:9382:1057:0]:50988 (errno: 97 - Address family not supported by protocol).
[2025-06-17 04:25:58,440] [INFO] [comm.py:675:init_distributed] cdb=None
[W617 04:25:58.151132574 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W617 04:25:58.151171336 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [fdff:ffff:1578:0:2cb9:24d:7155:0]:38906 (errno: 97 - Address family not supported by protocol).
[2025-06-17 04:25:58,511] [INFO] [comm.py:675:init_distributed] cdb=None
[W617 04:25:58.223592879 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W617 04:25:58.223626580 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [fdff:ffff:a473:0:2cb9:584b:a556:0]:36478 (errno: 97 - Address family not supported by protocol).
[2025-06-17 04:25:58,521] [INFO] [comm.py:675:init_distributed] cdb=None
[W617 04:25:58.232155083 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W617 04:25:58.232205175 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [fdff:ffff:187f:0:2ca9:dbfe:7964:0]:3345 (errno: 97 - Address family not supported by protocol).
Running on device: cuda:2 is_deepspeed: True
[2025-06-17 04:25:59,640] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
Running on device: cuda:1 is_deepspeed: True
Running on device: cuda:4 is_deepspeed: True
Running on device: cuda:5 is_deepspeed: True
Running on device: cuda:6 is_deepspeed: True
Running on device: cuda:3 is_deepspeed: True
Running on device: cuda:7 is_deepspeed: True
[2025-06-17 04:25:59,991] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 04:26:00,007] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 04:26:00,011] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 04:26:00,026] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 04:26:00,088] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 04:26:00,382] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 04:26:03,758] [INFO] [partition_parameters.py:360:__exit__] finished initializing model - num_params = 291, num_elems = 8.03B
Loading dataset: wikitext (20.0% of data)...
Dataset loaded: 360270 examples using column 'text'
Tokenizing dataset...
Map:   0%|          | 0/360270 [00:00<?, ? examples/s]Map:   0%|          | 1000/360270 [00:00<01:08, 5269.33 examples/s]Map:   1%|          | 2000/360270 [00:00<01:10, 5097.15 examples/s]Map:   1%|          | 3000/360270 [00:00<01:12, 4899.19 examples/s]Map:   1%|          | 4000/360270 [00:00<01:08, 5197.49 examples/s]Map:   1%|▏         | 5000/360270 [00:00<01:10, 5032.80 examples/s]Map:   2%|▏         | 6000/360270 [00:01<01:17, 4585.12 examples/s]Map:   2%|▏         | 7000/360270 [00:01<01:13, 4775.86 examples/s]Map:   2%|▏         | 8000/360270 [00:01<01:15, 4653.48 examples/s]Map:   2%|▏         | 9000/360270 [00:01<01:14, 4713.46 examples/s]Map:   3%|▎         | 10000/360270 [00:02<01:10, 4998.90 examples/s]Map:   3%|▎         | 11000/360270 [00:02<01:11, 4877.88 examples/s]Map:   3%|▎         | 12000/360270 [00:02<01:13, 4755.74 examples/s]Map:   4%|▎         | 13000/360270 [00:02<01:11, 4840.21 examples/s]Map:   4%|▍         | 14000/360270 [00:02<01:11, 4815.38 examples/s]Map:   4%|▍         | 15000/360270 [00:03<01:13, 4704.27 examples/s]Map:   4%|▍         | 16000/360270 [00:03<01:12, 4765.70 examples/s]Map:   5%|▍         | 17000/360270 [00:03<01:11, 4803.32 examples/s]Map:   5%|▍         | 18000/360270 [00:03<01:11, 4806.61 examples/s]Map:   5%|▌         | 19000/360270 [00:03<01:08, 4980.74 examples/s]Map:   6%|▌         | 20000/360270 [00:04<01:07, 5068.31 examples/s]Map:   6%|▌         | 21000/360270 [00:04<01:05, 5191.35 examples/s]Map:   6%|▌         | 22000/360270 [00:04<01:02, 5435.78 examples/s]Map:   6%|▋         | 23000/360270 [00:04<00:59, 5661.65 examples/s]Map:   7%|▋         | 24000/360270 [00:04<01:07, 4978.32 examples/s]Map:   7%|▋         | 25000/360270 [00:05<01:07, 4939.46 examples/s]Map:   7%|▋         | 26000/360270 [00:05<01:07, 4983.50 examples/s]Map:   7%|▋         | 27000/360270 [00:05<01:06, 5010.95 examples/s]Map:   8%|▊         | 28000/360270 [00:05<01:07, 4947.95 examples/s]Map:   8%|▊         | 29000/360270 [00:06<02:13, 2488.12 examples/s]Map:   8%|▊         | 30000/360270 [00:06<01:53, 2918.19 examples/s]Map:   9%|▊         | 31000/360270 [00:06<01:38, 3350.18 examples/s]Map:   9%|▉         | 32000/360270 [00:07<01:33, 3493.44 examples/s]Map:   9%|▉         | 33000/360270 [00:07<01:27, 3731.48 examples/s]Map:   9%|▉         | 34000/360270 [00:07<01:20, 4077.01 examples/s]Map:  10%|▉         | 35000/360270 [00:07<01:18, 4148.65 examples/s]Map:  10%|▉         | 36000/360270 [00:08<01:16, 4242.40 examples/s]Map:  10%|█         | 37000/360270 [00:08<01:14, 4342.73 examples/s]Map:  11%|█         | 38000/360270 [00:08<01:12, 4424.28 examples/s]Map:  11%|█         | 39000/360270 [00:08<01:11, 4467.41 examples/s]Map:  11%|█         | 40000/360270 [00:08<01:09, 4614.63 examples/s]Map:  11%|█▏        | 41000/360270 [00:09<01:09, 4590.02 examples/s]Map:  12%|█▏        | 42000/360270 [00:09<01:10, 4535.70 examples/s]Map:  12%|█▏        | 43000/360270 [00:09<01:09, 4564.99 examples/s]Map:  12%|█▏        | 44000/360270 [00:09<01:07, 4695.33 examples/s]Map:  12%|█▏        | 45000/360270 [00:09<01:07, 4678.61 examples/s]Map:  13%|█▎        | 46000/360270 [00:10<01:08, 4566.61 examples/s]Map:  13%|█▎        | 47000/360270 [00:10<01:22, 3803.71 examples/s]Map:  13%|█▎        | 48000/360270 [00:10<01:18, 4002.95 examples/s]Map:  14%|█▎        | 49000/360270 [00:11<01:13, 4236.47 examples/s]Map:  14%|█▍        | 50000/360270 [00:11<01:10, 4412.92 examples/s]Map:  14%|█▍        | 51000/360270 [00:11<01:09, 4481.92 examples/s]Map:  14%|█▍        | 52000/360270 [00:11<01:06, 4628.45 examples/s]Map:  15%|█▍        | 53000/360270 [00:11<01:08, 4480.09 examples/s]Map:  15%|█▍        | 54000/360270 [00:12<01:08, 4442.72 examples/s]Map:  15%|█▌        | 55000/360270 [00:12<01:08, 4488.79 examples/s]Map:  16%|█▌        | 56000/360270 [00:12<01:08, 4465.71 examples/s]Map:  16%|█▌        | 57000/360270 [00:12<01:07, 4511.15 examples/s]Map:  16%|█▌        | 58000/360270 [00:12<01:04, 4707.52 examples/s]Map:  16%|█▋        | 59000/360270 [00:13<01:03, 4748.73 examples/s]Map:  17%|█▋        | 60000/360270 [00:13<01:05, 4566.32 examples/s]Map:  17%|█▋        | 61000/360270 [00:13<01:02, 4750.85 examples/s]Map:  17%|█▋        | 62000/360270 [00:13<01:02, 4810.64 examples/s]Map:  17%|█▋        | 63000/360270 [00:14<01:03, 4716.00 examples/s]Map:  18%|█▊        | 64000/360270 [00:14<01:01, 4821.05 examples/s]Map:  18%|█▊        | 65000/360270 [00:14<01:00, 4852.23 examples/s]Map:  18%|█▊        | 66000/360270 [00:14<01:02, 4710.00 examples/s]Map:  19%|█▊        | 67000/360270 [00:14<01:02, 4707.29 examples/s]Map:  19%|█▉        | 68000/360270 [00:15<01:03, 4580.09 examples/s]Map:  19%|█▉        | 69000/360270 [00:15<01:02, 4680.84 examples/s]Map:  19%|█▉        | 70000/360270 [00:15<01:00, 4773.14 examples/s]Map:  20%|█▉        | 71000/360270 [00:16<01:39, 2894.43 examples/s]Map:  20%|█▉        | 72000/360270 [00:16<01:28, 3271.62 examples/s]Map:  20%|██        | 73000/360270 [00:16<01:19, 3597.20 examples/s]Map:  21%|██        | 74000/360270 [00:16<01:11, 3977.95 examples/s]Map:  21%|██        | 75000/360270 [00:16<01:07, 4197.97 examples/s]Map:  21%|██        | 76000/360270 [00:17<01:07, 4221.75 examples/s]Map:  21%|██▏       | 77000/360270 [00:17<01:05, 4311.09 examples/s]Map:  22%|██▏       | 78000/360270 [00:17<01:05, 4281.12 examples/s]Map:  22%|██▏       | 79000/360270 [00:17<01:05, 4306.10 examples/s]Map:  22%|██▏       | 80000/360270 [00:18<01:04, 4312.36 examples/s]Map:  22%|██▏       | 81000/360270 [00:18<01:06, 4191.52 examples/s]Map:  23%|██▎       | 82000/360270 [00:18<01:04, 4309.25 examples/s]Map:  23%|██▎       | 83000/360270 [00:18<01:05, 4250.61 examples/s]Map:  23%|██▎       | 84000/360270 [00:19<01:04, 4250.68 examples/s]Map:  24%|██▎       | 85000/360270 [00:19<01:04, 4246.19 examples/s]Map:  24%|██▍       | 86000/360270 [00:19<01:02, 4355.61 examples/s]Map:  24%|██▍       | 87000/360270 [00:19<01:01, 4429.84 examples/s]Map:  24%|██▍       | 88000/360270 [00:19<01:01, 4420.83 examples/s]Map:  25%|██▍       | 89000/360270 [00:20<01:03, 4251.34 examples/s]Map:  25%|██▍       | 90000/360270 [00:20<01:01, 4396.57 examples/s]Map:  25%|██▌       | 91000/360270 [00:20<00:59, 4524.37 examples/s]Map:  26%|██▌       | 92000/360270 [00:20<01:00, 4443.43 examples/s]Map:  26%|██▌       | 93000/360270 [00:21<00:58, 4555.97 examples/s]Map:  26%|██▌       | 94000/360270 [00:21<01:23, 3193.94 examples/s]Map:  26%|██▋       | 95000/360270 [00:21<01:17, 3441.93 examples/s]Map:  27%|██▋       | 96000/360270 [00:22<01:12, 3670.27 examples/s]Map:  27%|██▋       | 97000/360270 [00:22<01:08, 3852.57 examples/s]Map:  27%|██▋       | 98000/360270 [00:22<01:02, 4208.44 examples/s]Map:  27%|██▋       | 99000/360270 [00:22<00:58, 4452.63 examples/s]Map:  28%|██▊       | 100000/360270 [00:22<00:56, 4607.27 examples/s]Map:  28%|██▊       | 101000/360270 [00:23<00:58, 4451.56 examples/s]Map:  28%|██▊       | 102000/360270 [00:23<00:58, 4402.73 examples/s]Map:  29%|██▊       | 103000/360270 [00:23<00:57, 4469.67 examples/s]Map:  29%|██▉       | 104000/360270 [00:23<01:00, 4222.24 examples/s]Map:  29%|██▉       | 105000/360270 [00:24<01:00, 4250.50 examples/s]Map:  29%|██▉       | 106000/360270 [00:24<01:00, 4189.34 examples/s]Map:  30%|██▉       | 107000/360270 [00:24<00:59, 4251.31 examples/s]Map:  30%|██▉       | 108000/360270 [00:24<01:00, 4190.47 examples/s]Map:  30%|███       | 109000/360270 [00:25<00:58, 4282.49 examples/s]Map:  31%|███       | 110000/360270 [00:25<00:58, 4314.06 examples/s]Map:  31%|███       | 111000/360270 [00:25<00:55, 4476.59 examples/s]Map:  31%|███       | 112000/360270 [00:25<00:53, 4607.67 examples/s]Map:  31%|███▏      | 113000/360270 [00:26<01:22, 3002.79 examples/s]Map:  32%|███▏      | 114000/360270 [00:26<01:12, 3386.60 examples/s]Map:  32%|███▏      | 115000/360270 [00:26<01:04, 3784.24 examples/s]Map:  32%|███▏      | 116000/360270 [00:26<01:03, 3869.09 examples/s]Map:  32%|███▏      | 117000/360270 [00:27<01:01, 3967.92 examples/s]Map:  33%|███▎      | 118000/360270 [00:27<01:01, 3969.49 examples/s]Map:  33%|███▎      | 119000/360270 [00:27<01:00, 3975.94 examples/s]Map:  33%|███▎      | 120000/360270 [00:27<00:58, 4092.40 examples/s]Map:  34%|███▎      | 121000/360270 [00:28<00:57, 4186.58 examples/s]Map:  34%|███▍      | 122000/360270 [00:28<00:58, 4083.27 examples/s]Map:  34%|███▍      | 123000/360270 [00:28<00:58, 4047.11 examples/s]Map:  34%|███▍      | 124000/360270 [00:28<00:58, 4019.83 examples/s]Map:  35%|███▍      | 125000/360270 [00:29<00:55, 4222.42 examples/s]Map:  35%|███▍      | 126000/360270 [00:29<00:52, 4448.14 examples/s]Map:  35%|███▌      | 127000/360270 [00:29<00:49, 4670.83 examples/s]Map:  36%|███▌      | 128000/360270 [00:29<00:50, 4585.52 examples/s]Map:  36%|███▌      | 129000/360270 [00:29<00:51, 4455.21 examples/s]Map:  36%|███▌      | 130000/360270 [00:30<00:52, 4388.80 examples/s]Map:  36%|███▋      | 131000/360270 [00:30<00:51, 4433.65 examples/s]Map:  37%|███▋      | 132000/360270 [00:30<00:50, 4528.36 examples/s]Map:  37%|███▋      | 133000/360270 [00:30<00:48, 4700.14 examples/s]Map:  37%|███▋      | 134000/360270 [00:31<00:51, 4370.70 examples/s]Map:  37%|███▋      | 135000/360270 [00:31<00:52, 4327.51 examples/s]Map:  38%|███▊      | 136000/360270 [00:31<00:49, 4491.10 examples/s]Map:  38%|███▊      | 137000/360270 [00:31<00:48, 4620.00 examples/s]Map:  38%|███▊      | 138000/360270 [00:31<00:46, 4744.75 examples/s]Map:  39%|███▊      | 139000/360270 [00:32<00:46, 4741.52 examples/s]Map:  39%|███▉      | 140000/360270 [00:32<00:47, 4608.72 examples/s]Map:  39%|███▉      | 141000/360270 [00:32<00:47, 4634.31 examples/s]Map:  39%|███▉      | 142000/360270 [00:32<00:47, 4612.93 examples/s]Map:  40%|███▉      | 143000/360270 [00:33<00:48, 4499.66 examples/s]Map:  40%|███▉      | 144000/360270 [00:33<00:49, 4336.27 examples/s]Map:  40%|████      | 145000/360270 [00:33<00:46, 4584.42 examples/s]Map:  41%|████      | 146000/360270 [00:33<00:47, 4548.69 examples/s]Map:  41%|████      | 147000/360270 [00:33<00:48, 4381.61 examples/s]Map:  41%|████      | 148000/360270 [00:34<00:46, 4596.70 examples/s]Map:  41%|████▏     | 149000/360270 [00:34<00:45, 4621.14 examples/s]Map:  42%|████▏     | 150000/360270 [00:34<00:45, 4622.40 examples/s]Map:  42%|████▏     | 151000/360270 [00:34<00:45, 4616.22 examples/s]Map:  42%|████▏     | 152000/360270 [00:34<00:46, 4482.30 examples/s]Map:  42%|████▏     | 153000/360270 [00:35<00:45, 4576.93 examples/s]Map:  43%|████▎     | 154000/360270 [00:35<00:44, 4609.29 examples/s]Map:  43%|████▎     | 155000/360270 [00:36<01:10, 2929.12 examples/s]Map:  43%|████▎     | 156000/360270 [00:36<01:02, 3287.57 examples/s]Map:  44%|████▎     | 157000/360270 [00:36<00:56, 3619.46 examples/s]Map:  44%|████▍     | 158000/360270 [00:36<00:51, 3922.50 examples/s]Map:  44%|████▍     | 159000/360270 [00:36<00:49, 4072.05 examples/s]Map:  44%|████▍     | 160000/360270 [00:37<00:48, 4095.47 examples/s]Map:  45%|████▍     | 161000/360270 [00:37<00:48, 4069.50 examples/s]Map:  45%|████▍     | 162000/360270 [00:37<00:47, 4197.63 examples/s]Map:  45%|████▌     | 163000/360270 [00:37<00:45, 4313.56 examples/s]Map:  46%|████▌     | 164000/360270 [00:38<00:45, 4347.11 examples/s]Map:  46%|████▌     | 165000/360270 [00:38<00:43, 4441.92 examples/s]Map:  46%|████▌     | 166000/360270 [00:38<00:44, 4397.25 examples/s]Map:  46%|████▋     | 167000/360270 [00:38<00:45, 4292.10 examples/s]Map:  47%|████▋     | 168000/360270 [00:38<00:44, 4355.36 examples/s]Map:  47%|████▋     | 169000/360270 [00:39<00:43, 4391.99 examples/s]Map:  47%|████▋     | 170000/360270 [00:39<00:42, 4425.51 examples/s]Map:  47%|████▋     | 171000/360270 [00:39<00:43, 4319.08 examples/s]Map:  48%|████▊     | 172000/360270 [00:39<00:43, 4362.60 examples/s]Map:  48%|████▊     | 173000/360270 [00:40<00:43, 4329.67 examples/s]Map:  48%|████▊     | 174000/360270 [00:40<00:43, 4295.76 examples/s]Map:  49%|████▊     | 175000/360270 [00:40<00:44, 4143.64 examples/s]Map:  49%|████▉     | 176000/360270 [00:40<00:44, 4144.80 examples/s]Map:  49%|████▉     | 177000/360270 [00:41<00:43, 4221.78 examples/s]Map:  49%|████▉     | 178000/360270 [00:41<00:44, 4138.99 examples/s]Map:  50%|████▉     | 179000/360270 [00:41<00:46, 3913.24 examples/s]Map:  50%|████▉     | 180000/360270 [00:41<00:46, 3868.19 examples/s]Map:  50%|█████     | 181000/360270 [00:42<00:44, 3988.74 examples/s]Map:  51%|█████     | 182000/360270 [00:42<00:44, 4003.20 examples/s]Map:  51%|█████     | 183000/360270 [00:42<00:45, 3896.72 examples/s]Map:  51%|█████     | 184000/360270 [00:42<00:44, 3927.42 examples/s]Map:  51%|█████▏    | 185000/360270 [00:43<00:43, 4036.04 examples/s]Map:  52%|█████▏    | 186000/360270 [00:43<00:41, 4178.43 examples/s]Map:  52%|█████▏    | 187000/360270 [00:43<00:41, 4214.56 examples/s]Map:  52%|█████▏    | 188000/360270 [00:44<01:04, 2668.01 examples/s]Map:  52%|█████▏    | 189000/360270 [00:44<00:56, 3007.19 examples/s]Map:  53%|█████▎    | 190000/360270 [00:44<00:49, 3467.04 examples/s]Map:  53%|█████▎    | 191000/360270 [00:44<00:43, 3883.36 examples/s]Map:  53%|█████▎    | 192000/360270 [00:45<00:42, 3993.64 examples/s]Map:  54%|█████▎    | 193000/360270 [00:45<00:42, 3963.04 examples/s]Map:  54%|█████▍    | 194000/360270 [00:45<00:40, 4072.58 examples/s]Map:  54%|█████▍    | 195000/360270 [00:45<00:40, 4080.81 examples/s]Map:  54%|█████▍    | 196000/360270 [00:46<00:39, 4117.58 examples/s]Map:  55%|█████▍    | 197000/360270 [00:46<00:55, 2951.11 examples/s]Map:  55%|█████▍    | 198000/360270 [00:46<00:50, 3219.43 examples/s]Map:  55%|█████▌    | 199000/360270 [00:47<00:47, 3414.46 examples/s]Map:  56%|█████▌    | 200000/360270 [00:47<00:42, 3752.93 examples/s]Map:  56%|█████▌    | 201000/360270 [00:47<00:39, 3989.95 examples/s]Map:  56%|█████▌    | 202000/360270 [00:47<00:39, 4056.94 examples/s]Map:  56%|█████▋    | 203000/360270 [00:48<00:37, 4191.70 examples/s]Map:  57%|█████▋    | 204000/360270 [00:48<00:36, 4251.76 examples/s]Map:  57%|█████▋    | 205000/360270 [00:48<00:36, 4285.74 examples/s]Map:  57%|█████▋    | 206000/360270 [00:48<00:36, 4248.27 examples/s]Map:  57%|█████▋    | 207000/360270 [00:48<00:35, 4374.69 examples/s]Map:  58%|█████▊    | 208000/360270 [00:49<00:34, 4414.09 examples/s]Map:  58%|█████▊    | 209000/360270 [00:49<00:34, 4414.85 examples/s]Map:  58%|█████▊    | 210000/360270 [00:49<00:33, 4422.93 examples/s]Map:  59%|█████▊    | 211000/360270 [00:49<00:33, 4417.83 examples/s]Map:  59%|█████▉    | 212000/360270 [00:50<00:33, 4422.59 examples/s]Map:  59%|█████▉    | 213000/360270 [00:50<00:34, 4313.75 examples/s]Map:  59%|█████▉    | 214000/360270 [00:50<00:33, 4431.64 examples/s]Map:  60%|█████▉    | 215000/360270 [00:50<00:32, 4443.83 examples/s]Map:  60%|█████▉    | 216000/360270 [00:50<00:32, 4507.70 examples/s]Map:  60%|██████    | 217000/360270 [00:51<00:32, 4422.16 examples/s]Map:  61%|██████    | 218000/360270 [00:51<00:32, 4433.80 examples/s]Map:  61%|██████    | 219000/360270 [00:51<00:31, 4497.63 examples/s]Map:  61%|██████    | 220000/360270 [00:51<00:30, 4536.97 examples/s]Map:  61%|██████▏   | 221000/360270 [00:52<00:31, 4399.35 examples/s]Map:  62%|██████▏   | 222000/360270 [00:52<00:31, 4420.21 examples/s]Map:  62%|██████▏   | 223000/360270 [00:52<00:31, 4300.39 examples/s]Map:  62%|██████▏   | 224000/360270 [00:52<00:31, 4327.81 examples/s]Map:  62%|██████▏   | 225000/360270 [00:53<00:31, 4319.15 examples/s]Map:  63%|██████▎   | 226000/360270 [00:53<00:31, 4330.43 examples/s]Map:  63%|██████▎   | 227000/360270 [00:53<00:32, 4151.37 examples/s]Map:  63%|██████▎   | 228000/360270 [00:53<00:32, 4024.92 examples/s]Map:  64%|██████▎   | 229000/360270 [00:53<00:31, 4145.41 examples/s]Map:  64%|██████▍   | 230000/360270 [00:54<00:30, 4245.53 examples/s]Map:  64%|██████▍   | 231000/360270 [00:54<00:30, 4272.59 examples/s]Map:  64%|██████▍   | 232000/360270 [00:54<00:29, 4319.10 examples/s]Map:  65%|██████▍   | 233000/360270 [00:54<00:29, 4257.86 examples/s]Map:  65%|██████▍   | 234000/360270 [00:55<00:29, 4217.89 examples/s]Map:  65%|██████▌   | 235000/360270 [00:55<00:29, 4179.09 examples/s]Map:  66%|██████▌   | 236000/360270 [00:55<00:29, 4214.38 examples/s]Map:  66%|██████▌   | 237000/360270 [00:55<00:28, 4382.61 examples/s]Map:  66%|██████▌   | 238000/360270 [00:56<00:27, 4461.32 examples/s]Map:  66%|██████▋   | 239000/360270 [00:56<00:40, 3000.24 examples/s]Map:  67%|██████▋   | 240000/360270 [00:56<00:36, 3321.44 examples/s]Map:  67%|██████▋   | 241000/360270 [00:57<00:33, 3592.70 examples/s]Map:  67%|██████▋   | 242000/360270 [00:57<00:30, 3836.86 examples/s]Map:  67%|██████▋   | 243000/360270 [00:57<00:28, 4107.10 examples/s]Map:  68%|██████▊   | 244000/360270 [00:57<00:27, 4157.64 examples/s]Map:  68%|██████▊   | 245000/360270 [00:57<00:27, 4262.12 examples/s]Map:  68%|██████▊   | 246000/360270 [00:58<00:26, 4372.98 examples/s]Map:  69%|██████▊   | 247000/360270 [00:58<00:26, 4316.19 examples/s]Map:  69%|██████▉   | 248000/360270 [00:58<00:27, 4125.90 examples/s]Map:  69%|██████▉   | 249000/360270 [00:58<00:26, 4146.42 examples/s]Map:  69%|██████▉   | 250000/360270 [00:59<00:25, 4264.69 examples/s]Map:  70%|██████▉   | 251000/360270 [00:59<00:25, 4212.92 examples/s]Map:  70%|██████▉   | 252000/360270 [00:59<00:25, 4225.63 examples/s]Map:  70%|███████   | 253000/360270 [00:59<00:24, 4379.11 examples/s]Map:  71%|███████   | 254000/360270 [01:00<00:24, 4366.50 examples/s]Map:  71%|███████   | 255000/360270 [01:00<00:24, 4380.70 examples/s]Map:  71%|███████   | 256000/360270 [01:00<00:24, 4324.36 examples/s]Map:  71%|███████▏  | 257000/360270 [01:00<00:24, 4284.04 examples/s]Map:  72%|███████▏  | 258000/360270 [01:01<00:24, 4185.19 examples/s]Map:  72%|███████▏  | 259000/360270 [01:01<00:25, 3963.98 examples/s]Map:  72%|███████▏  | 260000/360270 [01:01<00:24, 4046.56 examples/s]Map:  72%|███████▏  | 261000/360270 [01:01<00:23, 4150.29 examples/s]Map:  73%|███████▎  | 262000/360270 [01:01<00:23, 4194.76 examples/s]Map:  73%|███████▎  | 263000/360270 [01:02<00:23, 4066.84 examples/s]Map:  73%|███████▎  | 264000/360270 [01:02<00:24, 3978.77 examples/s]Map:  74%|███████▎  | 265000/360270 [01:02<00:24, 3815.44 examples/s]Map:  74%|███████▍  | 266000/360270 [01:03<00:24, 3854.61 examples/s]Map:  74%|███████▍  | 267000/360270 [01:03<00:24, 3744.95 examples/s]Map:  74%|███████▍  | 268000/360270 [01:03<00:23, 3942.52 examples/s]Map:  75%|███████▍  | 269000/360270 [01:03<00:22, 4094.34 examples/s]Map:  75%|███████▍  | 270000/360270 [01:04<00:21, 4170.28 examples/s]Map:  75%|███████▌  | 271000/360270 [01:04<00:20, 4286.46 examples/s]Map:  75%|███████▌  | 272000/360270 [01:04<00:20, 4289.59 examples/s]Map:  76%|███████▌  | 273000/360270 [01:04<00:20, 4243.88 examples/s]Map:  76%|███████▌  | 274000/360270 [01:04<00:19, 4322.68 examples/s]Map:  76%|███████▋  | 275000/360270 [01:05<00:20, 4231.77 examples/s]Map:  77%|███████▋  | 276000/360270 [01:05<00:20, 4207.37 examples/s]Map:  77%|███████▋  | 277000/360270 [01:05<00:19, 4242.51 examples/s]Map:  77%|███████▋  | 278000/360270 [01:05<00:19, 4216.59 examples/s]Map:  77%|███████▋  | 279000/360270 [01:06<00:18, 4383.56 examples/s]Map:  78%|███████▊  | 280000/360270 [01:06<00:18, 4423.48 examples/s]Map:  78%|███████▊  | 281000/360270 [01:06<00:26, 2979.39 examples/s]Map:  78%|███████▊  | 282000/360270 [01:07<00:23, 3343.49 examples/s]Map:  79%|███████▊  | 283000/360270 [01:07<00:21, 3659.23 examples/s]Map:  79%|███████▉  | 284000/360270 [01:07<00:19, 3954.24 examples/s]Map:  79%|███████▉  | 285000/360270 [01:07<00:18, 4105.67 examples/s]Map:  79%|███████▉  | 286000/360270 [01:08<00:18, 4108.83 examples/s]Map:  80%|███████▉  | 287000/360270 [01:08<00:18, 4061.68 examples/s]Map:  80%|███████▉  | 288000/360270 [01:08<00:18, 3987.37 examples/s]Map:  80%|████████  | 289000/360270 [01:08<00:17, 3960.14 examples/s]Map:  80%|████████  | 290000/360270 [01:09<00:16, 4152.15 examples/s]Map:  81%|████████  | 291000/360270 [01:09<00:16, 4178.03 examples/s]Map:  81%|████████  | 292000/360270 [01:09<00:15, 4379.27 examples/s]Map:  81%|████████▏ | 293000/360270 [01:09<00:15, 4274.08 examples/s]Map:  82%|████████▏ | 294000/360270 [01:09<00:16, 4051.09 examples/s]Map:  82%|████████▏ | 295000/360270 [01:10<00:15, 4144.94 examples/s]Map:  82%|████████▏ | 296000/360270 [01:10<00:15, 4120.19 examples/s]Map:  82%|████████▏ | 297000/360270 [01:10<00:15, 4158.64 examples/s]Map:  83%|████████▎ | 298000/360270 [01:10<00:14, 4159.61 examples/s]Map:  83%|████████▎ | 299000/360270 [01:11<00:14, 4168.52 examples/s]Map:  83%|████████▎ | 300000/360270 [01:11<00:14, 4207.83 examples/s]Map:  84%|████████▎ | 301000/360270 [01:11<00:14, 4224.35 examples/s]Map:  84%|████████▍ | 302000/360270 [01:11<00:13, 4199.12 examples/s]Map:  84%|████████▍ | 303000/360270 [01:12<00:13, 4094.95 examples/s]Map:  84%|████████▍ | 304000/360270 [01:12<00:13, 4113.37 examples/s]Map:  85%|████████▍ | 305000/360270 [01:12<00:13, 4152.54 examples/s]Map:  85%|████████▍ | 306000/360270 [01:12<00:13, 4157.59 examples/s]Map:  85%|████████▌ | 307000/360270 [01:13<00:12, 4296.56 examples/s]Map:  85%|████████▌ | 308000/360270 [01:13<00:11, 4364.88 examples/s]Map:  86%|████████▌ | 309000/360270 [01:13<00:11, 4340.01 examples/s]Map:  86%|████████▌ | 310000/360270 [01:13<00:11, 4425.10 examples/s]Map:  86%|████████▋ | 311000/360270 [01:13<00:11, 4379.03 examples/s]Map:  87%|████████▋ | 312000/360270 [01:14<00:11, 4322.12 examples/s]Map:  87%|████████▋ | 313000/360270 [01:14<00:11, 4222.27 examples/s]Map:  87%|████████▋ | 314000/360270 [01:14<00:11, 4184.52 examples/s]Map:  87%|████████▋ | 315000/360270 [01:14<00:11, 4021.15 examples/s]Map:  88%|████████▊ | 316000/360270 [01:15<00:10, 4101.75 examples/s]Map:  88%|████████▊ | 317000/360270 [01:15<00:10, 4052.35 examples/s]Map:  88%|████████▊ | 318000/360270 [01:15<00:10, 3974.03 examples/s]Map:  89%|████████▊ | 319000/360270 [01:15<00:09, 4217.58 examples/s]Map:  89%|████████▉ | 320000/360270 [01:16<00:09, 4305.38 examples/s]Map:  89%|████████▉ | 321000/360270 [01:16<00:08, 4373.83 examples/s]Map:  89%|████████▉ | 322000/360270 [01:16<00:08, 4376.23 examples/s]Map:  90%|████████▉ | 323000/360270 [01:17<00:12, 2931.78 examples/s]Map:  90%|████████▉ | 324000/360270 [01:17<00:11, 3198.27 examples/s]Map:  90%|█████████ | 325000/360270 [01:17<00:10, 3497.19 examples/s]Map:  90%|█████████ | 326000/360270 [01:17<00:09, 3639.25 examples/s]Map:  91%|█████████ | 327000/360270 [01:18<00:08, 3773.21 examples/s]Map:  91%|█████████ | 328000/360270 [01:18<00:07, 4057.93 examples/s]Map:  91%|█████████▏| 329000/360270 [01:18<00:07, 4091.85 examples/s]Map:  92%|█████████▏| 330000/360270 [01:18<00:07, 4101.74 examples/s]Map:  92%|█████████▏| 331000/360270 [01:19<00:07, 4117.08 examples/s]Map:  92%|█████████▏| 332000/360270 [01:19<00:07, 3984.50 examples/s]Map:  92%|█████████▏| 333000/360270 [01:19<00:06, 3980.18 examples/s]Map:  93%|█████████▎| 334000/360270 [01:19<00:06, 4012.19 examples/s]Map:  93%|█████████▎| 335000/360270 [01:20<00:06, 4049.52 examples/s]Map:  93%|█████████▎| 336000/360270 [01:20<00:05, 4135.87 examples/s]Map:  94%|█████████▎| 337000/360270 [01:20<00:05, 4182.14 examples/s]Map:  94%|█████████▍| 338000/360270 [01:20<00:05, 4156.48 examples/s]Map:  94%|█████████▍| 339000/360270 [01:21<00:05, 4167.48 examples/s]Map:  94%|█████████▍| 340000/360270 [01:21<00:04, 4265.56 examples/s]Map:  95%|█████████▍| 341000/360270 [01:21<00:04, 4238.39 examples/s]Map:  95%|█████████▍| 342000/360270 [01:21<00:04, 4079.95 examples/s]Map:  95%|█████████▌| 343000/360270 [01:22<00:04, 3810.63 examples/s]Map:  95%|█████████▌| 344000/360270 [01:22<00:04, 3822.63 examples/s]Map:  96%|█████████▌| 345000/360270 [01:22<00:04, 3811.15 examples/s]Map:  96%|█████████▌| 346000/360270 [01:22<00:03, 3869.95 examples/s]Map:  96%|█████████▋| 347000/360270 [01:23<00:03, 3879.78 examples/s]Map:  97%|█████████▋| 348000/360270 [01:23<00:03, 3912.20 examples/s]Map:  97%|█████████▋| 349000/360270 [01:23<00:02, 3932.08 examples/s]Map:  97%|█████████▋| 350000/360270 [01:23<00:02, 3935.65 examples/s]Map:  97%|█████████▋| 351000/360270 [01:24<00:02, 3639.23 examples/s][2025-06-17 04:27:29,982] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
Map:  98%|█████████▊| 352000/360270 [01:24<00:02, 3856.49 examples/s]Map:  98%|█████████▊| 353000/360270 [01:24<00:01, 3778.61 examples/s]Map:  98%|█████████▊| 354000/360270 [01:24<00:01, 3820.76 examples/s]Map:  99%|█████████▊| 355000/360270 [01:25<00:01, 4013.08 examples/s]Map:  99%|█████████▉| 356000/360270 [01:25<00:01, 3838.50 examples/s]Map:  99%|█████████▉| 357000/360270 [01:25<00:00, 3957.53 examples/s][2025-06-17 04:27:31,541] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
Map:  99%|█████████▉| 358000/360270 [01:25<00:00, 4063.57 examples/s]Map: 100%|█████████▉| 359000/360270 [01:26<00:00, 4143.23 examples/s]Map: 100%|█████████▉| 360000/360270 [01:26<00:00, 4280.39 examples/s]Map: 100%|██████████| 360270/360270 [01:26<00:00, 4168.14 examples/s]
Tokenization complete. Dataset ready with 360270 examples.
[2025-06-17 04:27:32,119] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.17.1+e440506b, git-hash=e440506b, git-branch=master
[2025-06-17 04:27:32,119] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 04:27:32,125] [INFO] [engine.py:1325:_configure_distributed_model] ********** distributed groups summary **********
	 self.dp_world_size=8
	 self.mp_world_size=1
	 self.seq_dp_world_size=8
	 self.sequence_parallel_size=1
***********************************************
[2025-06-17 04:27:32,126] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-06-17 04:27:32,127] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-06-17 04:27:32,127] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-06-17 04:27:32,134] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-06-17 04:27:32,134] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2025-06-17 04:27:32,134] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2025-06-17 04:27:32,134] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2025-06-17 04:27:32,398] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2025-06-17 04:27:32,398] [INFO] [utils.py:782:see_memory_usage] MA 1.87 GB         Max_MA 3.83 GB         CA 4.8 GB         Max_CA 5 GB 
[2025-06-17 04:27:32,399] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 81.79 GB, percent = 4.5%
[2025-06-17 04:27:32,400] [INFO] [stage3.py:170:__init__] Reduce bucket size 500000000
[2025-06-17 04:27:32,400] [INFO] [stage3.py:171:__init__] Prefetch bucket size 50000000
[2025-06-17 04:27:32,547] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 04:27:32,720] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-06-17 04:27:32,720] [INFO] [utils.py:782:see_memory_usage] MA 1.87 GB         Max_MA 1.87 GB         CA 4.8 GB         Max_CA 5 GB 
[2025-06-17 04:27:32,720] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 81.82 GB, percent = 4.5%
Parameter Offload - Persistent parameters statistics: param_count = 65, numel = 266240
[2025-06-17 04:27:33,038] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-06-17 04:27:33,038] [INFO] [utils.py:782:see_memory_usage] MA 1.87 GB         Max_MA 1.87 GB         CA 4.8 GB         Max_CA 5 GB 
[2025-06-17 04:27:33,039] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 81.83 GB, percent = 4.5%
[2025-06-17 04:27:33,110] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 04:27:33,135] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 04:27:33,156] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 04:27:33,306] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2025-06-17 04:27:33,307] [INFO] [utils.py:782:see_memory_usage] MA 1.87 GB         Max_MA 1.87 GB         CA 4.8 GB         Max_CA 5 GB 
[2025-06-17 04:27:33,307] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 81.84 GB, percent = 4.5%
[2025-06-17 04:27:33,753] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 8
[2025-06-17 04:27:36,067] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 10
[2025-06-17 04:27:36,068] [INFO] [utils.py:782:see_memory_usage] MA 1.87 GB         Max_MA 1.87 GB         CA 1.87 GB         Max_CA 5 GB 
[2025-06-17 04:27:36,068] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 81.86 GB, percent = 4.5%
[2025-06-17 04:27:36,192] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2025-06-17 04:27:36,193] [INFO] [utils.py:782:see_memory_usage] MA 1.87 GB         Max_MA 1.87 GB         CA 1.87 GB         Max_CA 2 GB 
[2025-06-17 04:27:36,193] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 81.86 GB, percent = 4.5%
[2025-06-17 04:27:36,349] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2025-06-17 04:27:36,350] [INFO] [utils.py:782:see_memory_usage] MA 5.61 GB         Max_MA 5.76 GB         CA 5.83 GB         Max_CA 6 GB 
[2025-06-17 04:27:36,350] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 81.86 GB, percent = 4.5%
[2025-06-17 04:27:36,481] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-06-17 04:27:36,481] [INFO] [utils.py:782:see_memory_usage] MA 5.61 GB         Max_MA 5.61 GB         CA 5.83 GB         Max_CA 6 GB 
[2025-06-17 04:27:36,481] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 81.82 GB, percent = 4.5%
[2025-06-17 04:27:36,603] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-06-17 04:27:36,604] [INFO] [utils.py:782:see_memory_usage] MA 5.61 GB         Max_MA 6.0 GB         CA 6.22 GB         Max_CA 6 GB 
[2025-06-17 04:27:36,604] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 81.82 GB, percent = 4.5%
[2025-06-17 04:27:36,604] [INFO] [stage3.py:534:_setup_for_real_optimizer] optimizer state initialized
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'> optimizer: <class 'accelerate.utils.deepspeed.DeepSpeedOptimizerWrapper'>
[2025-06-17 04:27:36,679] [INFO] [engine.py:3953:compile] Compiling deepcompile=True backend=inductor
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'> optimizer: <class 'accelerate.utils.deepspeed.DeepSpeedOptimizerWrapper'>
[2025-06-17 04:27:36,682] [INFO] [engine.py:3953:compile] Compiling deepcompile=True backend=inductor
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'> optimizer: <class 'accelerate.utils.deepspeed.DeepSpeedOptimizerWrapper'>
[2025-06-17 04:27:36,682] [INFO] [engine.py:3953:compile] Compiling deepcompile=True backend=inductor
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'> optimizer: <class 'accelerate.utils.deepspeed.DeepSpeedOptimizerWrapper'>
[2025-06-17 04:27:36,685] [INFO] [engine.py:3953:compile] Compiling deepcompile=True backend=inductor
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'> optimizer: <class 'accelerate.utils.deepspeed.DeepSpeedOptimizerWrapper'>
[2025-06-17 04:27:36,688] [INFO] [engine.py:3953:compile] Compiling deepcompile=True backend=inductor
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'> optimizer: <class 'accelerate.utils.deepspeed.DeepSpeedOptimizerWrapper'>
[2025-06-17 04:27:36,688] [INFO] [engine.py:3953:compile] Compiling deepcompile=True backend=inductor
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'> optimizer: <class 'accelerate.utils.deepspeed.DeepSpeedOptimizerWrapper'>
[2025-06-17 04:27:36,690] [INFO] [engine.py:3953:compile] Compiling deepcompile=True backend=inductor
[2025-06-17 04:27:36,871] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-06-17 04:27:36,872] [INFO] [utils.py:782:see_memory_usage] MA 8.41 GB         Max_MA 10.37 GB         CA 10.98 GB         Max_CA 11 GB 
[2025-06-17 04:27:36,872] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 81.92 GB, percent = 4.5%
[2025-06-17 04:27:36,872] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2025-06-17 04:27:36,872] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-06-17 04:27:36,872] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-06-17 04:27:36,872] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-06], mom=[(0.9, 0.999)]
[2025-06-17 04:27:36,873] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True
[2025-06-17 04:27:36,873] [INFO] [config.py:921:print] DeepSpeedEngine configuration:
[2025-06-17 04:27:36,873] [INFO] [config.py:925:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-06-17 04:27:36,873] [INFO] [config.py:925:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-06-17 04:27:36,873] [INFO] [config.py:925:print]   amp_enabled .................. False
[2025-06-17 04:27:36,873] [INFO] [config.py:925:print]   amp_params ................... False
[2025-06-17 04:27:36,873] [INFO] [config.py:925:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-06-17 04:27:36,873] [INFO] [config.py:925:print]   bfloat16_config .............. enabled=True immediate_grad_update=False check_grad_overflow=False
[2025-06-17 04:27:36,873] [INFO] [config.py:925:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   checkpoint_parallel_write_pipeline  False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   checkpoint_tag_validation_enabled  True
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   checkpoint_tag_validation_fail  False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x79bc5c16b4d0>
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   communication_data_type ...... None
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   compile_config ............... deepcompile=True free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   curriculum_enabled_legacy .... False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   curriculum_params_legacy ..... False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   data_efficiency_enabled ...... False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   dataloader_drop_last ......... False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   disable_allgather ............ False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   dump_state ................... False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   eigenvalue_enabled ........... False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   eigenvalue_gas_boundary_resolution  1
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   eigenvalue_layer_num ......... 0
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   eigenvalue_max_iter .......... 100
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   eigenvalue_stability ......... 1e-06
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   eigenvalue_tol ............... 0.01
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   eigenvalue_verbose ........... False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   elasticity_enabled ........... False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   float16_config ............... enabled=False auto_cast=False loss_scale=0.0 initial_scale_power=16 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   global_rank .................. 0
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   grad_accum_dtype ............. None
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   gradient_accumulation_steps .. 4
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   gradient_clipping ............ 1.0
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   gradient_predivide_factor .... 1.0
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   graph_harvesting ............. False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   load_universal_checkpoint .... False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   memory_breakdown ............. False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   mics_hierarchial_params_gather  False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   mics_shard_size .............. -1
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   optimizer_legacy_fusion ...... False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   optimizer_name ............... None
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   optimizer_params ............. None
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   pld_enabled .................. False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   pld_params ................... False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   prescale_gradients ........... False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   scheduler_name ............... None
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   scheduler_params ............. None
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   seq_parallel_communication_data_type  torch.float32
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   sparse_attention ............. None
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   sparse_gradients_enabled ..... False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   steps_per_print .............. inf
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   timers_config ................ enabled=True synchronized=True
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   train_batch_size ............. 32
[2025-06-17 04:27:36,874] [INFO] [config.py:925:print]   train_micro_batch_size_per_gpu  1
[2025-06-17 04:27:36,875] [INFO] [config.py:925:print]   use_data_before_expert_parallel_  False
[2025-06-17 04:27:36,875] [INFO] [config.py:925:print]   use_node_local_storage ....... False
[2025-06-17 04:27:36,875] [INFO] [config.py:925:print]   wall_clock_breakdown ......... False
[2025-06-17 04:27:36,875] [INFO] [config.py:925:print]   weight_quantization_config ... None
[2025-06-17 04:27:36,875] [INFO] [config.py:925:print]   world_size ................... 8
[2025-06-17 04:27:36,875] [INFO] [config.py:925:print]   zero_allow_untested_optimizer  True
[2025-06-17 04:27:36,875] [INFO] [config.py:925:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-06-17 04:27:36,875] [INFO] [config.py:925:print]   zero_enabled ................. True
[2025-06-17 04:27:36,875] [INFO] [config.py:925:print]   zero_force_ds_cpu_optimizer .. True
[2025-06-17 04:27:36,875] [INFO] [config.py:925:print]   zero_optimization_stage ...... 3
[2025-06-17 04:27:36,875] [INFO] [config.py:911:print_user_config]   json = {
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "sub_group_size": 1.000000e+08
    }, 
    "compile": {
        "deepcompile": true, 
        "offload_activation": false, 
        "offload_opt_states": false, 
        "double_buffer": true, 
        "symmetric_memory": false, 
        "free_activation": false, 
        "debug_log": false, 
        "sync_before_reduce": false, 
        "sync_after_reduce": false
    }, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 1, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'> optimizer: <class 'accelerate.utils.deepspeed.DeepSpeedOptimizerWrapper'>
wandb: Currently logged in as: mtanaka (orangewandb) to https://msaip.wandb.io. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.20.1
wandb: Run data is saved locally in /home/mtanaka/work/dc/ds_verify_loss/wandb/run-20250617_042737-rbjz5u9t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Meta-Llama-3-8B_np8ds1Binductorz3L0bs1seq512acc4ac0c1dc1pass_none_os0T20250617042736
wandb: ⭐️ View project at https://msaip.wandb.io/orangewandb/ds-verify-loss
wandb: 🚀 View run at https://msaip.wandb.io/orangewandb/ds-verify-loss/runs/rbjz5u9t
[2025-06-17 04:27:38,326] [INFO] [engine.py:3953:compile] Compiling deepcompile=True backend=inductor
Using /home/mtanaka/.cache/torch_extensions/py311_cu126 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/mtanaka/.cache/torch_extensions/py311_cu126/dc/build.ninja...
/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module dc...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module dc...
Time to load dc op: 2.289318799972534 seconds
Using /home/mtanaka/.cache/torch_extensions/py311_cu126 as PyTorch extensions root...
Using /home/mtanaka/.cache/torch_extensions/py311_cu126 as PyTorch extensions root...
Using /home/mtanaka/.cache/torch_extensions/py311_cu126 as PyTorch extensions root...
Using /home/mtanaka/.cache/torch_extensions/py311_cu126 as PyTorch extensions root...
Using /home/mtanaka/.cache/torch_extensions/py311_cu126 as PyTorch extensions root...
Using /home/mtanaka/.cache/torch_extensions/py311_cu126 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/mtanaka/.cache/torch_extensions/py311_cu126/dc/build.ninja...
/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module dc...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module dc...
Time to load dc op: 2.3334062099456787 seconds
Loading extension module dc...
Loading extension module dc...
Time to load dc op: 2.430262327194214 seconds
Time to load dc op: 2.4270787239074707 seconds
Loading extension module dc...
Loading extension module dc...
Loading extension module dc...
Time to load dc op: 2.4288840293884277 seconds
Time to load dc op: 2.426034688949585 seconds
Time to load dc op: 2.4275379180908203 seconds
Using /home/mtanaka/.cache/torch_extensions/py311_cu126 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/mtanaka/.cache/torch_extensions/py311_cu126/dc/build.ninja...
/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module dc...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module dc...
Time to load dc op: 2.2341320514678955 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Launching compile passes: global_steps=0 passes=[<function add_z3_gather_release at 0x79bc8bdfede0>]
/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Launching compile passes: global_steps=5 passes=[<function add_z3_gather_release at 0x79bc8bdfede0>, <function schedule_prefetch at 0x79bc8bdff380>, <function selective_gather at 0x79bc8bdff560>]
schedule_prefetch graph_id=133846240974288 max_mem=135109394432.0 available_memory=125032202240 memory_allocated=20987051008 max_allocated=20987051520 total_param_size=16060522496 margin=0.1
size: 0, avg_duration: 0
size: 16, avg_duration: 2.674759889487177e-05
size: 32, avg_duration: 2.26715983444592e-05
size: 64, avg_duration: 2.267519994347822e-05
size: 128, avg_duration: 2.414720074739307e-05
size: 256, avg_duration: 2.2976799300522543e-05
size: 512, avg_duration: 2.240920002805069e-05
size: 1024, avg_duration: 2.256480001960881e-05
size: 2048, avg_duration: 2.2185999114299193e-05
size: 4096, avg_duration: 2.582720117061399e-05
size: 8192, avg_duration: 2.2709598852088675e-05
size: 16384, avg_duration: 2.225360003649257e-05
size: 32768, avg_duration: 2.2836398784420453e-05
size: 65536, avg_duration: 2.312519973202143e-05
size: 131072, avg_duration: 2.3027201677905396e-05
size: 262144, avg_duration: 2.3099999452824704e-05
size: 524288, avg_duration: 2.36331998166861e-05
size: 1048576, avg_duration: 2.464439967297949e-05
size: 2097152, avg_duration: 2.550679892010521e-05
size: 4194304, avg_duration: 3.651839870144613e-05
size: 8388608, avg_duration: 4.5914799557067454e-05
size: 16777216, avg_duration: 0.00014977400132920593
size: 33554432, avg_duration: 0.0001222224091179669
size: 67108864, avg_duration: 0.00020523999410215765
size: 134217728, avg_duration: 0.00037774760858155787
size: 268435456, avg_duration: 0.0007191176409833133
size: 536870912, avg_duration: 0.0013915596064180136
size: 1073741824, avg_duration: 0.002720481716096401
size: 2147483648, avg_duration: 0.005353268701583147
schedule_prefetch graph_id=133846240974288 max_mem=135109394432.0 available_memory=123245428736 memory_allocated=20987011584 max_allocated=20987011584 total_param_size=15009849344 margin=0.1
selective_gather graph_id=133846240974288 max_mem=33345472512 fwd_max_mem=33345472512 bwd_max_mem=33274087936
selective_gather max_mem=33345472512 total_mem=150121545728 MEM_MARGIN=0.1 available_mem=101763918643.2
Set persistent: 215 size: 8192 persistent_mem: 8192 shape: torch.Size([4096])
Set persistent: 81 size: 8192 persistent_mem: 16384 shape: torch.Size([4096])
Set persistent: 107 size: 8192 persistent_mem: 24576 shape: torch.Size([4096])
Set persistent: 45 size: 8192 persistent_mem: 32768 shape: torch.Size([4096])
Set persistent: 8 size: 8192 persistent_mem: 40960 shape: torch.Size([4096])
Set persistent: 90 size: 8192 persistent_mem: 49152 shape: torch.Size([4096])
Set persistent: 197 size: 8192 persistent_mem: 57344 shape: torch.Size([4096])
Set persistent: 98 size: 8192 persistent_mem: 65536 shape: torch.Size([4096])
Set persistent: 260 size: 8192 persistent_mem: 73728 shape: torch.Size([4096])
Set persistent: 288 size: 8192 persistent_mem: 81920 shape: torch.Size([4096])
Set persistent: 9 size: 8192 persistent_mem: 90112 shape: torch.Size([4096])
Set persistent: 287 size: 8192 persistent_mem: 98304 shape: torch.Size([4096])
Set persistent: 116 size: 8192 persistent_mem: 106496 shape: torch.Size([4096])
Set persistent: 117 size: 8192 persistent_mem: 114688 shape: torch.Size([4096])
Set persistent: 35 size: 8192 persistent_mem: 122880 shape: torch.Size([4096])
Set persistent: 36 size: 8192 persistent_mem: 131072 shape: torch.Size([4096])
Set persistent: 17 size: 8192 persistent_mem: 139264 shape: torch.Size([4096])
Set persistent: 233 size: 8192 persistent_mem: 147456 shape: torch.Size([4096])
Set persistent: 251 size: 8192 persistent_mem: 155648 shape: torch.Size([4096])
Set persistent: 27 size: 8192 persistent_mem: 163840 shape: torch.Size([4096])
Set persistent: 125 size: 8192 persistent_mem: 172032 shape: torch.Size([4096])
Set persistent: 26 size: 8192 persistent_mem: 180224 shape: torch.Size([4096])
Set persistent: 18 size: 8192 persistent_mem: 188416 shape: torch.Size([4096])
Set persistent: 234 size: 8192 persistent_mem: 196608 shape: torch.Size([4096])
Set persistent: 62 size: 8192 persistent_mem: 204800 shape: torch.Size([4096])
Set persistent: 153 size: 8192 persistent_mem: 212992 shape: torch.Size([4096])
Set persistent: 63 size: 8192 persistent_mem: 221184 shape: torch.Size([4096])
Set persistent: 71 size: 8192 persistent_mem: 229376 shape: torch.Size([4096])
Set persistent: 44 size: 8192 persistent_mem: 237568 shape: torch.Size([4096])
Set persistent: 54 size: 8192 persistent_mem: 245760 shape: torch.Size([4096])
Set persistent: 225 size: 8192 persistent_mem: 253952 shape: torch.Size([4096])
Set persistent: 135 size: 8192 persistent_mem: 262144 shape: torch.Size([4096])
Set persistent: 198 size: 8192 persistent_mem: 270336 shape: torch.Size([4096])
Set persistent: 53 size: 8192 persistent_mem: 278528 shape: torch.Size([4096])
Set persistent: 179 size: 8192 persistent_mem: 286720 shape: torch.Size([4096])
Set persistent: 252 size: 8192 persistent_mem: 294912 shape: torch.Size([4096])
Set persistent: 152 size: 8192 persistent_mem: 303104 shape: torch.Size([4096])
Set persistent: 72 size: 8192 persistent_mem: 311296 shape: torch.Size([4096])
Set persistent: 261 size: 8192 persistent_mem: 319488 shape: torch.Size([4096])
Set persistent: 126 size: 8192 persistent_mem: 327680 shape: torch.Size([4096])
Set persistent: 189 size: 8192 persistent_mem: 335872 shape: torch.Size([4096])
Set persistent: 80 size: 8192 persistent_mem: 344064 shape: torch.Size([4096])
Set persistent: 89 size: 8192 persistent_mem: 352256 shape: torch.Size([4096])
Set persistent: 171 size: 8192 persistent_mem: 360448 shape: torch.Size([4096])
Set persistent: 270 size: 8192 persistent_mem: 368640 shape: torch.Size([4096])
Set persistent: 243 size: 8192 persistent_mem: 376832 shape: torch.Size([4096])
Set persistent: 143 size: 8192 persistent_mem: 385024 shape: torch.Size([4096])
Set persistent: 99 size: 8192 persistent_mem: 393216 shape: torch.Size([4096])
Set persistent: 108 size: 8192 persistent_mem: 401408 shape: torch.Size([4096])
Set persistent: 207 size: 8192 persistent_mem: 409600 shape: torch.Size([4096])
Set persistent: 289 size: 8192 persistent_mem: 417792 shape: torch.Size([4096])
Set persistent: 134 size: 8192 persistent_mem: 425984 shape: torch.Size([4096])
Set persistent: 269 size: 8192 persistent_mem: 434176 shape: torch.Size([4096])
Set persistent: 162 size: 8192 persistent_mem: 442368 shape: torch.Size([4096])
Set persistent: 144 size: 8192 persistent_mem: 450560 shape: torch.Size([4096])
Set persistent: 216 size: 8192 persistent_mem: 458752 shape: torch.Size([4096])
Set persistent: 161 size: 8192 persistent_mem: 466944 shape: torch.Size([4096])
Set persistent: 279 size: 8192 persistent_mem: 475136 shape: torch.Size([4096])
Set persistent: 180 size: 8192 persistent_mem: 483328 shape: torch.Size([4096])
Set persistent: 206 size: 8192 persistent_mem: 491520 shape: torch.Size([4096])
Set persistent: 170 size: 8192 persistent_mem: 499712 shape: torch.Size([4096])
Set persistent: 224 size: 8192 persistent_mem: 507904 shape: torch.Size([4096])
Set persistent: 188 size: 8192 persistent_mem: 516096 shape: torch.Size([4096])
Set persistent: 242 size: 8192 persistent_mem: 524288 shape: torch.Size([4096])
Set persistent: 278 size: 8192 persistent_mem: 532480 shape: torch.Size([4096])
Set persistent: 128 size: 8388608 persistent_mem: 8921088 shape: torch.Size([1024, 4096])
Set persistent: 164 size: 8388608 persistent_mem: 17309696 shape: torch.Size([1024, 4096])
Set persistent: 201 size: 8388608 persistent_mem: 25698304 shape: torch.Size([1024, 4096])
Set persistent: 273 size: 8388608 persistent_mem: 34086912 shape: torch.Size([1024, 4096])
Set persistent: 282 size: 8388608 persistent_mem: 42475520 shape: torch.Size([1024, 4096])
Set persistent: 147 size: 8388608 persistent_mem: 50864128 shape: torch.Size([1024, 4096])
Set persistent: 110 size: 8388608 persistent_mem: 59252736 shape: torch.Size([1024, 4096])
Set persistent: 38 size: 8388608 persistent_mem: 67641344 shape: torch.Size([1024, 4096])
Set persistent: 3 size: 8388608 persistent_mem: 76029952 shape: torch.Size([1024, 4096])
Set persistent: 165 size: 8388608 persistent_mem: 84418560 shape: torch.Size([1024, 4096])
Set persistent: 12 size: 8388608 persistent_mem: 92807168 shape: torch.Size([1024, 4096])
Set persistent: 254 size: 8388608 persistent_mem: 101195776 shape: torch.Size([1024, 4096])
Set persistent: 2 size: 8388608 persistent_mem: 109584384 shape: torch.Size([1024, 4096])
Set persistent: 227 size: 8388608 persistent_mem: 117972992 shape: torch.Size([1024, 4096])
Set persistent: 174 size: 8388608 persistent_mem: 126361600 shape: torch.Size([1024, 4096])
Set persistent: 138 size: 8388608 persistent_mem: 134750208 shape: torch.Size([1024, 4096])
Set persistent: 255 size: 8388608 persistent_mem: 143138816 shape: torch.Size([1024, 4096])
Set persistent: 20 size: 8388608 persistent_mem: 151527424 shape: torch.Size([1024, 4096])
Set persistent: 21 size: 8388608 persistent_mem: 159916032 shape: torch.Size([1024, 4096])
Set persistent: 84 size: 8388608 persistent_mem: 168304640 shape: torch.Size([1024, 4096])
Set persistent: 66 size: 8388608 persistent_mem: 176693248 shape: torch.Size([1024, 4096])
Set persistent: 30 size: 8388608 persistent_mem: 185081856 shape: torch.Size([1024, 4096])
Set persistent: 29 size: 8388608 persistent_mem: 193470464 shape: torch.Size([1024, 4096])
Set persistent: 102 size: 8388608 persistent_mem: 201859072 shape: torch.Size([1024, 4096])
Set persistent: 11 size: 8388608 persistent_mem: 210247680 shape: torch.Size([1024, 4096])
Set persistent: 209 size: 8388608 persistent_mem: 218636288 shape: torch.Size([1024, 4096])
Set persistent: 57 size: 8388608 persistent_mem: 227024896 shape: torch.Size([1024, 4096])
Set persistent: 245 size: 8388608 persistent_mem: 235413504 shape: torch.Size([1024, 4096])
Set persistent: 92 size: 8388608 persistent_mem: 243802112 shape: torch.Size([1024, 4096])
Set persistent: 48 size: 8388608 persistent_mem: 252190720 shape: torch.Size([1024, 4096])
Set persistent: 101 size: 8388608 persistent_mem: 260579328 shape: torch.Size([1024, 4096])
Set persistent: 119 size: 8388608 persistent_mem: 268967936 shape: torch.Size([1024, 4096])
Set persistent: 65 size: 8388608 persistent_mem: 277356544 shape: torch.Size([1024, 4096])
Set persistent: 47 size: 8388608 persistent_mem: 285745152 shape: torch.Size([1024, 4096])
Set persistent: 246 size: 8388608 persistent_mem: 294133760 shape: torch.Size([1024, 4096])
Set persistent: 228 size: 8388608 persistent_mem: 302522368 shape: torch.Size([1024, 4096])
Set persistent: 173 size: 8388608 persistent_mem: 310910976 shape: torch.Size([1024, 4096])
Set persistent: 210 size: 8388608 persistent_mem: 319299584 shape: torch.Size([1024, 4096])
Set persistent: 218 size: 8388608 persistent_mem: 327688192 shape: torch.Size([1024, 4096])
Set persistent: 137 size: 8388608 persistent_mem: 336076800 shape: torch.Size([1024, 4096])
Set persistent: 111 size: 8388608 persistent_mem: 344465408 shape: torch.Size([1024, 4096])
Set persistent: 281 size: 8388608 persistent_mem: 352854016 shape: torch.Size([1024, 4096])
Set persistent: 191 size: 8388608 persistent_mem: 361242624 shape: torch.Size([1024, 4096])
Set persistent: 39 size: 8388608 persistent_mem: 369631232 shape: torch.Size([1024, 4096])
Set persistent: 75 size: 8388608 persistent_mem: 378019840 shape: torch.Size([1024, 4096])
Set persistent: 192 size: 8388608 persistent_mem: 386408448 shape: torch.Size([1024, 4096])
Set persistent: 83 size: 8388608 persistent_mem: 394797056 shape: torch.Size([1024, 4096])
Set persistent: 182 size: 8388608 persistent_mem: 403185664 shape: torch.Size([1024, 4096])
Set persistent: 263 size: 8388608 persistent_mem: 411574272 shape: torch.Size([1024, 4096])
Set persistent: 272 size: 8388608 persistent_mem: 419962880 shape: torch.Size([1024, 4096])
Set persistent: 120 size: 8388608 persistent_mem: 428351488 shape: torch.Size([1024, 4096])
Set persistent: 93 size: 8388608 persistent_mem: 436740096 shape: torch.Size([1024, 4096])
Set persistent: 155 size: 8388608 persistent_mem: 445128704 shape: torch.Size([1024, 4096])
Set persistent: 236 size: 8388608 persistent_mem: 453517312 shape: torch.Size([1024, 4096])
Set persistent: 74 size: 8388608 persistent_mem: 461905920 shape: torch.Size([1024, 4096])
Set persistent: 56 size: 8388608 persistent_mem: 470294528 shape: torch.Size([1024, 4096])
Set persistent: 183 size: 8388608 persistent_mem: 478683136 shape: torch.Size([1024, 4096])
Set persistent: 264 size: 8388608 persistent_mem: 487071744 shape: torch.Size([1024, 4096])
Set persistent: 237 size: 8388608 persistent_mem: 495460352 shape: torch.Size([1024, 4096])
Set persistent: 146 size: 8388608 persistent_mem: 503848960 shape: torch.Size([1024, 4096])
Set persistent: 219 size: 8388608 persistent_mem: 512237568 shape: torch.Size([1024, 4096])
Set persistent: 156 size: 8388608 persistent_mem: 520626176 shape: torch.Size([1024, 4096])
Set persistent: 200 size: 8388608 persistent_mem: 529014784 shape: torch.Size([1024, 4096])
Set persistent: 129 size: 8388608 persistent_mem: 537403392 shape: torch.Size([1024, 4096])
Set persistent: 238 size: 33554432 persistent_mem: 570957824 shape: torch.Size([4096, 4096])
Set persistent: 127 size: 33554432 persistent_mem: 604512256 shape: torch.Size([4096, 4096])
Set persistent: 157 size: 33554432 persistent_mem: 638066688 shape: torch.Size([4096, 4096])
Set persistent: 244 size: 33554432 persistent_mem: 671621120 shape: torch.Size([4096, 4096])
Set persistent: 91 size: 33554432 persistent_mem: 705175552 shape: torch.Size([4096, 4096])
Set persistent: 109 size: 33554432 persistent_mem: 738729984 shape: torch.Size([4096, 4096])
Set persistent: 211 size: 33554432 persistent_mem: 772284416 shape: torch.Size([4096, 4096])
Set persistent: 1 size: 33554432 persistent_mem: 805838848 shape: torch.Size([4096, 4096])
Set persistent: 154 size: 33554432 persistent_mem: 839393280 shape: torch.Size([4096, 4096])
Set persistent: 229 size: 33554432 persistent_mem: 872947712 shape: torch.Size([4096, 4096])
Set persistent: 118 size: 33554432 persistent_mem: 906502144 shape: torch.Size([4096, 4096])
Set persistent: 28 size: 33554432 persistent_mem: 940056576 shape: torch.Size([4096, 4096])
Set persistent: 37 size: 33554432 persistent_mem: 973611008 shape: torch.Size([4096, 4096])
Set persistent: 4 size: 33554432 persistent_mem: 1007165440 shape: torch.Size([4096, 4096])
Set persistent: 49 size: 33554432 persistent_mem: 1040719872 shape: torch.Size([4096, 4096])
Set persistent: 208 size: 33554432 persistent_mem: 1074274304 shape: torch.Size([4096, 4096])
Set persistent: 22 size: 33554432 persistent_mem: 1107828736 shape: torch.Size([4096, 4096])
Set persistent: 31 size: 33554432 persistent_mem: 1141383168 shape: torch.Size([4096, 4096])
Set persistent: 193 size: 33554432 persistent_mem: 1174937600 shape: torch.Size([4096, 4096])
Set persistent: 94 size: 33554432 persistent_mem: 1208492032 shape: torch.Size([4096, 4096])
Set persistent: 10 size: 33554432 persistent_mem: 1242046464 shape: torch.Size([4096, 4096])
Set persistent: 181 size: 33554432 persistent_mem: 1275600896 shape: torch.Size([4096, 4096])
Set persistent: 121 size: 33554432 persistent_mem: 1309155328 shape: torch.Size([4096, 4096])
Set persistent: 283 size: 33554432 persistent_mem: 1342709760 shape: torch.Size([4096, 4096])
Set persistent: 280 size: 33554432 persistent_mem: 1376264192 shape: torch.Size([4096, 4096])
Set persistent: 226 size: 33554432 persistent_mem: 1409818624 shape: torch.Size([4096, 4096])
Set persistent: 82 size: 33554432 persistent_mem: 1443373056 shape: torch.Size([4096, 4096])
Set persistent: 64 size: 33554432 persistent_mem: 1476927488 shape: torch.Size([4096, 4096])
Set persistent: 13 size: 33554432 persistent_mem: 1510481920 shape: torch.Size([4096, 4096])
Set persistent: 199 size: 33554432 persistent_mem: 1544036352 shape: torch.Size([4096, 4096])
Set persistent: 73 size: 33554432 persistent_mem: 1577590784 shape: torch.Size([4096, 4096])
Set persistent: 112 size: 33554432 persistent_mem: 1611145216 shape: torch.Size([4096, 4096])
Set persistent: 100 size: 33554432 persistent_mem: 1644699648 shape: torch.Size([4096, 4096])
Set persistent: 85 size: 33554432 persistent_mem: 1678254080 shape: torch.Size([4096, 4096])
Set persistent: 202 size: 33554432 persistent_mem: 1711808512 shape: torch.Size([4096, 4096])
Set persistent: 55 size: 33554432 persistent_mem: 1745362944 shape: torch.Size([4096, 4096])
Set persistent: 58 size: 33554432 persistent_mem: 1778917376 shape: torch.Size([4096, 4096])
Set persistent: 19 size: 33554432 persistent_mem: 1812471808 shape: torch.Size([4096, 4096])
Set persistent: 265 size: 33554432 persistent_mem: 1846026240 shape: torch.Size([4096, 4096])
Set persistent: 40 size: 33554432 persistent_mem: 1879580672 shape: torch.Size([4096, 4096])
Set persistent: 46 size: 33554432 persistent_mem: 1913135104 shape: torch.Size([4096, 4096])
Set persistent: 103 size: 33554432 persistent_mem: 1946689536 shape: torch.Size([4096, 4096])
Set persistent: 217 size: 33554432 persistent_mem: 1980243968 shape: torch.Size([4096, 4096])
Set persistent: 136 size: 33554432 persistent_mem: 2013798400 shape: torch.Size([4096, 4096])
Set persistent: 76 size: 33554432 persistent_mem: 2047352832 shape: torch.Size([4096, 4096])
Set persistent: 271 size: 33554432 persistent_mem: 2080907264 shape: torch.Size([4096, 4096])
Set persistent: 262 size: 33554432 persistent_mem: 2114461696 shape: torch.Size([4096, 4096])
Set persistent: 166 size: 33554432 persistent_mem: 2148016128 shape: torch.Size([4096, 4096])
Set persistent: 253 size: 33554432 persistent_mem: 2181570560 shape: torch.Size([4096, 4096])
Set persistent: 139 size: 33554432 persistent_mem: 2215124992 shape: torch.Size([4096, 4096])
Set persistent: 235 size: 33554432 persistent_mem: 2248679424 shape: torch.Size([4096, 4096])
Set persistent: 175 size: 33554432 persistent_mem: 2282233856 shape: torch.Size([4096, 4096])
Set persistent: 190 size: 33554432 persistent_mem: 2315788288 shape: torch.Size([4096, 4096])
Set persistent: 130 size: 33554432 persistent_mem: 2349342720 shape: torch.Size([4096, 4096])
Set persistent: 67 size: 33554432 persistent_mem: 2382897152 shape: torch.Size([4096, 4096])
Set persistent: 274 size: 33554432 persistent_mem: 2416451584 shape: torch.Size([4096, 4096])
Set persistent: 172 size: 33554432 persistent_mem: 2450006016 shape: torch.Size([4096, 4096])
Set persistent: 220 size: 33554432 persistent_mem: 2483560448 shape: torch.Size([4096, 4096])
Set persistent: 148 size: 33554432 persistent_mem: 2517114880 shape: torch.Size([4096, 4096])
Set persistent: 256 size: 33554432 persistent_mem: 2550669312 shape: torch.Size([4096, 4096])
Set persistent: 247 size: 33554432 persistent_mem: 2584223744 shape: torch.Size([4096, 4096])
Set persistent: 145 size: 33554432 persistent_mem: 2617778176 shape: torch.Size([4096, 4096])
Set persistent: 163 size: 33554432 persistent_mem: 2651332608 shape: torch.Size([4096, 4096])
Set persistent: 184 size: 33554432 persistent_mem: 2684887040 shape: torch.Size([4096, 4096])
Set persistent: 88 size: 117440512 persistent_mem: 2802327552 shape: torch.Size([4096, 14336])
Set persistent: 14 size: 117440512 persistent_mem: 2919768064 shape: torch.Size([14336, 4096])
Set persistent: 51 size: 117440512 persistent_mem: 3037208576 shape: torch.Size([14336, 4096])
Set persistent: 275 size: 117440512 persistent_mem: 3154649088 shape: torch.Size([14336, 4096])
Set persistent: 214 size: 117440512 persistent_mem: 3272089600 shape: torch.Size([4096, 14336])
Set persistent: 185 size: 117440512 persistent_mem: 3389530112 shape: torch.Size([14336, 4096])
Set persistent: 69 size: 117440512 persistent_mem: 3506970624 shape: torch.Size([14336, 4096])
Set persistent: 277 size: 117440512 persistent_mem: 3624411136 shape: torch.Size([4096, 14336])
Set persistent: 86 size: 117440512 persistent_mem: 3741851648 shape: torch.Size([14336, 4096])
Set persistent: 15 size: 117440512 persistent_mem: 3859292160 shape: torch.Size([14336, 4096])
Set persistent: 115 size: 117440512 persistent_mem: 3976732672 shape: torch.Size([4096, 14336])
Set persistent: 16 size: 117440512 persistent_mem: 4094173184 shape: torch.Size([4096, 14336])
Set persistent: 25 size: 117440512 persistent_mem: 4211613696 shape: torch.Size([4096, 14336])
Set persistent: 204 size: 117440512 persistent_mem: 4329054208 shape: torch.Size([14336, 4096])
Set persistent: 159 size: 117440512 persistent_mem: 4446494720 shape: torch.Size([14336, 4096])
Set persistent: 114 size: 117440512 persistent_mem: 4563935232 shape: torch.Size([14336, 4096])
Set persistent: 194 size: 117440512 persistent_mem: 4681375744 shape: torch.Size([14336, 4096])
Set persistent: 78 size: 117440512 persistent_mem: 4798816256 shape: torch.Size([14336, 4096])
Set persistent: 105 size: 117440512 persistent_mem: 4916256768 shape: torch.Size([14336, 4096])
Set persistent: 196 size: 117440512 persistent_mem: 5033697280 shape: torch.Size([4096, 14336])
Set persistent: 205 size: 117440512 persistent_mem: 5151137792 shape: torch.Size([4096, 14336])
Set persistent: 23 size: 117440512 persistent_mem: 5268578304 shape: torch.Size([14336, 4096])
Set persistent: 5 size: 117440512 persistent_mem: 5386018816 shape: torch.Size([14336, 4096])
Set persistent: 33 size: 117440512 persistent_mem: 5503459328 shape: torch.Size([14336, 4096])
Set persistent: 6 size: 117440512 persistent_mem: 5620899840 shape: torch.Size([14336, 4096])
Set persistent: 24 size: 117440512 persistent_mem: 5738340352 shape: torch.Size([14336, 4096])
Set persistent: 241 size: 117440512 persistent_mem: 5855780864 shape: torch.Size([4096, 14336])
Set persistent: 106 size: 117440512 persistent_mem: 5973221376 shape: torch.Size([4096, 14336])
Set persistent: 141 size: 117440512 persistent_mem: 6090661888 shape: torch.Size([14336, 4096])
Set persistent: 222 size: 117440512 persistent_mem: 6208102400 shape: torch.Size([14336, 4096])
Set persistent: 79 size: 117440512 persistent_mem: 6325542912 shape: torch.Size([4096, 14336])
Set persistent: 177 size: 117440512 persistent_mem: 6442983424 shape: torch.Size([14336, 4096])
Set persistent: 77 size: 117440512 persistent_mem: 6560423936 shape: torch.Size([14336, 4096])
Set persistent: 41 size: 117440512 persistent_mem: 6677864448 shape: torch.Size([14336, 4096])
Set persistent: 7 size: 117440512 persistent_mem: 6795304960 shape: torch.Size([4096, 14336])
Set persistent: 59 size: 117440512 persistent_mem: 6912745472 shape: torch.Size([14336, 4096])
Set persistent: 195 size: 117440512 persistent_mem: 7030185984 shape: torch.Size([14336, 4096])
Set persistent: 32 size: 117440512 persistent_mem: 7147626496 shape: torch.Size([14336, 4096])
Set persistent: 34 size: 117440512 persistent_mem: 7265067008 shape: torch.Size([4096, 14336])
Set persistent: 240 size: 117440512 persistent_mem: 7382507520 shape: torch.Size([14336, 4096])
Set persistent: 42 size: 117440512 persistent_mem: 7499948032 shape: torch.Size([14336, 4096])
Set persistent: 50 size: 117440512 persistent_mem: 7617388544 shape: torch.Size([14336, 4096])
Set persistent: 123 size: 117440512 persistent_mem: 7734829056 shape: torch.Size([14336, 4096])
Set persistent: 284 size: 117440512 persistent_mem: 7852269568 shape: torch.Size([14336, 4096])
Set persistent: 122 size: 117440512 persistent_mem: 7969710080 shape: torch.Size([14336, 4096])
Set persistent: 52 size: 117440512 persistent_mem: 8087150592 shape: torch.Size([4096, 14336])
Set persistent: 259 size: 117440512 persistent_mem: 8204591104 shape: torch.Size([4096, 14336])
Set persistent: 61 size: 117440512 persistent_mem: 8322031616 shape: torch.Size([4096, 14336])
Set persistent: 43 size: 117440512 persistent_mem: 8439472128 shape: torch.Size([4096, 14336])
Set persistent: 230 size: 117440512 persistent_mem: 8556912640 shape: torch.Size([14336, 4096])
Set persistent: 239 size: 117440512 persistent_mem: 8674353152 shape: torch.Size([14336, 4096])
Set persistent: 70 size: 117440512 persistent_mem: 8791793664 shape: torch.Size([4096, 14336])
Set persistent: 87 size: 117440512 persistent_mem: 8909234176 shape: torch.Size([14336, 4096])
Set persistent: 131 size: 117440512 persistent_mem: 9026674688 shape: torch.Size([14336, 4096])
Set persistent: 142 size: 117440512 persistent_mem: 9144115200 shape: torch.Size([4096, 14336])
Set persistent: 221 size: 117440512 persistent_mem: 9261555712 shape: torch.Size([14336, 4096])
Set persistent: 276 size: 117440512 persistent_mem: 9378996224 shape: torch.Size([14336, 4096])
Set persistent: 95 size: 117440512 persistent_mem: 9496436736 shape: torch.Size([14336, 4096])
Set persistent: 169 size: 117440512 persistent_mem: 9613877248 shape: torch.Size([4096, 14336])
Set persistent: 133 size: 117440512 persistent_mem: 9731317760 shape: torch.Size([4096, 14336])
Set persistent: 96 size: 117440512 persistent_mem: 9848758272 shape: torch.Size([14336, 4096])
Set persistent: 285 size: 117440512 persistent_mem: 9966198784 shape: torch.Size([14336, 4096])
Set persistent: 249 size: 117440512 persistent_mem: 10083639296 shape: torch.Size([14336, 4096])
Set persistent: 151 size: 117440512 persistent_mem: 10201079808 shape: torch.Size([4096, 14336])
Set persistent: 68 size: 117440512 persistent_mem: 10318520320 shape: torch.Size([14336, 4096])
Set persistent: 124 size: 117440512 persistent_mem: 10435960832 shape: torch.Size([4096, 14336])
Set persistent: 132 size: 117440512 persistent_mem: 10553401344 shape: torch.Size([14336, 4096])
Set persistent: 160 size: 117440512 persistent_mem: 10670841856 shape: torch.Size([4096, 14336])
Set persistent: 250 size: 117440512 persistent_mem: 10788282368 shape: torch.Size([4096, 14336])
Set persistent: 203 size: 117440512 persistent_mem: 10905722880 shape: torch.Size([14336, 4096])
Set persistent: 268 size: 117440512 persistent_mem: 11023163392 shape: torch.Size([4096, 14336])
Set persistent: 104 size: 117440512 persistent_mem: 11140603904 shape: torch.Size([14336, 4096])
Set persistent: 176 size: 117440512 persistent_mem: 11258044416 shape: torch.Size([14336, 4096])
Set persistent: 286 size: 117440512 persistent_mem: 11375484928 shape: torch.Size([4096, 14336])
Set persistent: 223 size: 117440512 persistent_mem: 11492925440 shape: torch.Size([4096, 14336])
Set persistent: 113 size: 117440512 persistent_mem: 11610365952 shape: torch.Size([14336, 4096])
Set persistent: 187 size: 117440512 persistent_mem: 11727806464 shape: torch.Size([4096, 14336])
Set persistent: 140 size: 117440512 persistent_mem: 11845246976 shape: torch.Size([14336, 4096])
Set persistent: 149 size: 117440512 persistent_mem: 11962687488 shape: torch.Size([14336, 4096])
Set persistent: 60 size: 117440512 persistent_mem: 12080128000 shape: torch.Size([14336, 4096])
Set persistent: 248 size: 117440512 persistent_mem: 12197568512 shape: torch.Size([14336, 4096])
Set persistent: 97 size: 117440512 persistent_mem: 12315009024 shape: torch.Size([4096, 14336])
Set persistent: 167 size: 117440512 persistent_mem: 12432449536 shape: torch.Size([14336, 4096])
Set persistent: 257 size: 117440512 persistent_mem: 12549890048 shape: torch.Size([14336, 4096])
Set persistent: 186 size: 117440512 persistent_mem: 12667330560 shape: torch.Size([14336, 4096])
Set persistent: 168 size: 117440512 persistent_mem: 12784771072 shape: torch.Size([14336, 4096])
Set persistent: 212 size: 117440512 persistent_mem: 12902211584 shape: torch.Size([14336, 4096])
Set persistent: 158 size: 117440512 persistent_mem: 13019652096 shape: torch.Size([14336, 4096])
Set persistent: 150 size: 117440512 persistent_mem: 13137092608 shape: torch.Size([14336, 4096])
Set persistent: 258 size: 117440512 persistent_mem: 13254533120 shape: torch.Size([14336, 4096])
Set persistent: 267 size: 117440512 persistent_mem: 13371973632 shape: torch.Size([14336, 4096])
Set persistent: 213 size: 117440512 persistent_mem: 13489414144 shape: torch.Size([14336, 4096])
Set persistent: 178 size: 117440512 persistent_mem: 13606854656 shape: torch.Size([4096, 14336])
Set persistent: 231 size: 117440512 persistent_mem: 13724295168 shape: torch.Size([14336, 4096])
Set persistent: 266 size: 117440512 persistent_mem: 13841735680 shape: torch.Size([14336, 4096])
Set persistent: 232 size: 117440512 persistent_mem: 13959176192 shape: torch.Size([4096, 14336])
Set persistent: 290 size: 1050673152 persistent_mem: 15009849344 shape: torch.Size([128256, 4096])
Set persistent: 0 size: 1050673152 persistent_mem: 16060522496 shape: torch.Size([128256, 4096])
Epoch 1, Step 40, Loss: 10.776421 sync: True time: 0.47135019302368164 alloc_mem: 34421386240 peak_mem: 38596064256
Epoch 1, Step 80, Loss: 7.446548 sync: True time: 0.47081971168518066 alloc_mem: 34421386240 peak_mem: 38596064256
Epoch 1, Step 120, Loss: 6.881239 sync: True time: 0.4696493148803711 alloc_mem: 34421386240 peak_mem: 38596064256
Epoch 1, Step 160, Loss: 4.318827 sync: True time: 0.47110795974731445 alloc_mem: 34421386240 peak_mem: 38596064256
Epoch 1, Step 200, Loss: 2.176278 sync: True time: 0.4693305492401123 alloc_mem: 34421386240 peak_mem: 38596064256
Epoch 1, Step 240, Loss: 2.754859 sync: True time: 0.46877050399780273 alloc_mem: 34421386240 peak_mem: 38596064256
Epoch 1, Step 280, Loss: 1.192891 sync: True time: 0.46887731552124023 alloc_mem: 34421386240 peak_mem: 38596064256
Epoch 1, Step 320, Loss: 0.367286 sync: True time: 0.46882104873657227 alloc_mem: 34421386240 peak_mem: 38596064256
Epoch 1, Step 360, Loss: 2.283031 sync: True time: 0.46877217292785645 alloc_mem: 34421386240 peak_mem: 38596064256
Epoch 1, Step 400, Loss: 1.628428 sync: True time: 0.46946191787719727 alloc_mem: 34421386240 peak_mem: 38596064256
meta-llama/Meta-Llama-3-8B ds=True np=8 batch_size=1 seq=512 zero_stage=3 acc=4 ac=False compile=True backend=inductor deepcompile=True passes=None compile_time=58.16309404373169 iteration time: 0.4704 alloc_mem: 34421386240 peak_mem: 38596064256
wandb: uploading history steps 6-9, summary, console lines 346-349
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      summary/average_iteration_time ▁
wandb:            summary/compile_time_sum ▁
wandb: summary/final_cuda_memory_allocated ▁
wandb:      summary/final_cuda_memory_peak ▁
wandb:                 summary/total_steps ▁
wandb:        system/cuda_memory_allocated ▁▁▁▁▁▁▁▁▁▁
wandb:             system/cuda_memory_peak ▁▁▁▁▁▁▁▁▁▁
wandb:               timing/iteration_time █▇▃█▃▁▂▁▁▃
wandb:                         train/epoch ▁▁▁▁▁▁▁▁▁▁
wandb:                   train/global_step ▁▂▃▃▄▅▆▆▇█
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▆▅▄▂▃▂▁▂▂
wandb: 
wandb: Run summary:
wandb:      summary/average_iteration_time 0.47044
wandb:            summary/compile_time_sum 58.16309
wandb: summary/final_cuda_memory_allocated 34421386240
wandb:      summary/final_cuda_memory_peak 38596064256
wandb:                 summary/total_steps 400
wandb:        system/cuda_memory_allocated 34421386240
wandb:             system/cuda_memory_peak 38596064256
wandb:               timing/iteration_time 0.46995
wandb:                         train/epoch 1
wandb:                   train/global_step 400
wandb:                 train/learning_rate 0.0
wandb:                          train/loss 1.62843
wandb: 
wandb: 🚀 View run Meta-Llama-3-8B_np8ds1Binductorz3L0bs1seq512acc4ac0c1dc1pass_none_os0T20250617042736 at: https://msaip.wandb.io/orangewandb/ds-verify-loss/runs/rbjz5u9t
wandb: ⭐️ View project at: https://msaip.wandb.io/orangewandb/ds-verify-loss
wandb: Synced 8 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250617_042737-rbjz5u9t/logs
